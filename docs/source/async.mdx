# 비동기 추론

[SmolVLA](https://huggingface.co/papers/2506.01844)를 통해 실제 로봇에서 추론을 실행하는 새로운 방법인 **액션 예측과 액션 실행의 분리**를 도입했습니다.
이 튜토리얼에서는 SmolVLA의 미세 조정 버전과 LeRobot이 지원하는 모든 정책을 사용하여 비동기 추론(_async inference_)을 사용하는 방법을 보여줍니다.
**LeRobot이 지원하는 모든 정책**으로 비동기 추론을 시도해보세요!

**배울 내용:**

1. 비동기 추론이 중요한 이유와 더 전통적인 순차 추론과 비교하는 방법.
2. 같은 머신에서 `PolicyServer`를 시작하고 `RobotClient`를 연결하는 방법, 심지어 네트워크를 통해서도.
3. 로봇 및 정책에 대한 주요 매개변수(`actions_per_chunk`, `chunk_size_threshold`)를 조정하는 방법.

막히면 [Discord 커뮤니티](https://discord.gg/s3KuuzsPFb)로 오세요!

요약: _비동기 추론_을 사용하면 정책 서버가 이미 다음 액션 청크를 계산하는 동안 로봇이 계속 행동하여 "추론 대기" 지연을 제거하고 더 부드럽고 반응적인 동작을 가능하게 합니다.
이것은 로봇이 정책이 다음 액션 청크를 계산하는 동안 유휴 상태로 있는 동기 추론(sync)과 근본적으로 다릅니다.

---

## 비동기 추론 시작하기

비동기 추론에 대한 자세한 정보는 [블로그 게시물](https://huggingface.co/blog/async-robot-inference)에서 읽을 수 있습니다. 이 가이드는 환경에서 비동기 추론을 빠르게 설정하고 실행하는 데 도움을 주기 위해 설계되었습니다.

먼저 비동기 추론을 실행하는 데 필요한 추가 종속성을 설치하기 위해 `async` 태그와 함께 `lerobot`을 설치합니다.

```shell
pip install -e ".[async]"
```

그런 다음 클라이언트가 연결할 호스트 주소와 포트를 지정하여 정책 서버를 시작합니다(한 터미널 또는 별도의 머신에서).
다음을 실행하여 정책 서버를 시작할 수 있습니다:

```shell
python -m lerobot.async_inference.policy_server \
     --host=127.0.0.1 \
     --port=8080
```

이렇게 하면 `127.0.0.1:8080`(`localhost`, 포트 8080)에서 수신 대기하는 정책 서버가 시작됩니다. 이 단계에서 정책 서버는 비어 있습니다. 실행할 정책 및 매개변수와 관련된 모든 정보는 클라이언트와의 첫 번째 핸드셰이크 중에 지정됩니다. 클라이언트를 시작합니다:

```shell
python -m lerobot.async_inference.robot_client \
    --server_address=127.0.0.1:8080 \ # SERVER: 정책 서버의 호스트 주소와 포트
    --robot.type=so100_follower \ # ROBOT: 로봇 유형
    --robot.port=/dev/tty.usbmodem585A0076841 \ # ROBOT: 로봇 포트
    --robot.id=follower_so100 \ # ROBOT: 캘리브레이션 파일을 로드하기 위한 로봇 ID
    --robot.cameras="{ laptop: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}, phone: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}}" \ # POLICY: 정책이 기대하는 키와 일치하는 키로 프레임을 획득하는 데 사용되는 카메라
    --task="dummy" \ # POLICY: 정책을 실행할 작업 (`Fold my t-shirt`). `act`와 같은 모든 정책에 대해 반드시 정의되지는 않습니다.
    --policy_type=your_policy_type \ # POLICY: 실행할 정책 유형 (smolvla, act 등)
    --pretrained_name_or_path=user/model \ # POLICY: 실행할 체크포인트에 대한 서버의 모델 이름/경로 (예: lerobot/smolvla_base)
    --policy_device=mps \ # POLICY: 서버에서 정책을 실행할 디바이스
    --actions_per_chunk=50 \ # POLICY: 한 번에 출력할 액션 수
    --chunk_size_threshold=0.5 \ # CLIENT: 서버에 새 관찰을 보내기 전 청크 크기의 임계값
    --aggregate_fn_name=weighted_average \ # CLIENT: 겹치는 부분에서 액션을 집계하는 함수
    --debug_visualize_queue_size=True # CLIENT: 런타임에 큐 크기를 시각화할지 여부
```

요약하면 다음에 대한 지침을 지정해야 합니다:

- `SERVER`: 정책 서버의 주소와 포트
- `ROBOT`: 연결할 로봇 유형, 연결할 포트 및 로봇의 로컬 `id`
- `POLICY`: 실행할 정책 유형 및 실행할 체크포인트에 대한 서버의 모델 이름/경로. 또한 서버가 사용해야 하는 디바이스와 한 번에 출력할 액션 수(정책 최대 액션 값으로 제한됨)를 지정해야 합니다.
- `CLIENT`: 서버에 새 관찰을 보내기 전 청크 크기의 임계값과 겹치는 부분에서 액션을 집계하는 함수. 선택적으로 런타임에 큐 크기를 시각화하여 `CLIENT` 매개변수를 조정하는 데 도움을 줄 수도 있습니다.

중요한 점은,

- `actions_per_chunk`와 `chunk_size_threshold`는 설정에 맞게 조정해야 하는 주요 매개변수입니다.
- `aggregate_fn_name`은 겹치는 부분에서 액션을 집계하는 함수입니다. 함수 레지스트리에 새 함수를 추가하거나 `robot_client.py`에 자체 함수를 추가할 수 있습니다([여기](NOTE:addlinktoLOC) 참조).
- `debug_visualize_queue_size`는 `CLIENT` 매개변수를 조정하는 데 유용한 도구입니다.

## 완료! 이제 로봇이 움직이는 것을 볼 수 있어야 합니다 😉

## 비동기 vs. 동기 추론

동기 추론은 액션 청크 예측과 액션 실행을 교대로 수행하는 데 의존합니다. 이는 본질적으로 _유휴 프레임_, 즉 로봇이 정책의 출력(새 액션 청크)을 기다리며 유휴 상태로 있는 프레임을 초래합니다.
결과적으로 추론은 사용 가능한 액션 부족으로 인해 로봇이 단순히 행동을 멈추는 명백한 실시간 지연에 시달립니다.
로봇 공학 모델의 크기가 증가함에 따라 이 문제는 더욱 심각해질 위험이 있습니다.

<p align="center">
  <img
    src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/sync.png"
    width="80%"
  ></img>
</p>
<p align="center">
  <i>동기 추론</i>은 정책이 다음 액션 청크를 계산하는 동안 로봇을 유휴 상태로 만듭니다.
</p>

이를 극복하기 위해 우리는 액션 계획과 실행이 분리되어 (1) 더 높은 적응성과 가장 중요하게는 (2) 유휴 프레임이 없는 패러다임인 비동기 추론을 설계했습니다.
결정적으로 비동기 추론을 사용하면 현재 청크가 소진되기 _전에_ 다음 액션 청크가 계산되어 유휴 상태가 없습니다.
더 높은 적응성은 겹치는 부분에서 다른 액션 청크를 집계하여 최신 계획과 더 긴밀한 제어 루프를 얻음으로써 보장됩니다.

<p align="center">
  <img
    src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/async.png"
    width="80%"
  ></img>
</p>
<p align="center">
  <i>비동기 추론</i>은 현재 청크가 소진되기 전에 다음 청크가 계산되므로 유휴 상태가 없습니다.
</p>

---

## 정책 서버 시작

정책 서버는 로봇 클라이언트에서 들어오는 관찰과 인터페이스하는 `PreTrainedPolicy`의 래퍼입니다.
정책 서버는 로봇 클라이언트와 정책 서버 간의 초기 핸드셰이크에서 지정된 요청된 정책으로 채워지는 빈 컨테이너로 초기화됩니다.
따라서 정책 서버를 시작하는 것은 호스트 주소와 포트를 지정하는 것만큼 쉽습니다. 정책 서버를 로봇 클라이언트와 같은 머신에서 실행하는 경우 호스트 주소로 `localhost`를 사용할 수 있습니다.

<hfoptions id="start_policy_server">
<hfoption id="Command">
```bash
python -m lerobot.async_inference.policy_server \
     --host=127.0.0.1 \
     --port=8080
```
</hfoption>
<hfoption id="API example">

<!-- prettier-ignore-start -->
```python
from lerobot.async_inference.configs import PolicyServerConfig
from lerobot.async_inference.policy_server import serve

config = PolicyServerConfig(
    host="localhost",
    port=8080,
)
serve(config)
```
<!-- prettier-ignore-end -->

</hfoption>
</hfoptions>

이렇게 하면 관련 `RobotClient`의 들어오는 연결을 위해 `localhost:8080`에서 수신 대기하며, 첫 번째 클라이언트-서버 핸드셰이크 중에 실행할 정책을 전달합니다.

---

## 로봇 클라이언트 시작

`RobotClient`는 `Robot` 인스턴스의 래퍼로, `RobotClient`가 (원격일 수 있는) `PolicyServer`에 연결합니다.
`RobotClient`는 `PolicyServer`로 관찰을 스트리밍하고, 서버에서 추론을 실행하여 얻은 액션 청크를 수신합니다(로봇 컨트롤러보다 더 나은 계산 리소스를 가지고 있다고 가정합니다).

<hfoptions id="start_robot_client">
<hfoption id="Command">
```bash
python -m lerobot.async_inference.robot_client \
    --server_address=127.0.0.1:8080 \ # SERVER: 정책 서버의 호스트 주소와 포트
    --robot.type=so100_follower \ # ROBOT: 로봇 유형
    --robot.port=/dev/tty.usbmodem585A0076841 \ # ROBOT: 로봇 포트
    --robot.id=follower_so100 \ # ROBOT: 캘리브레이션 파일을 로드하기 위한 로봇 ID
    --robot.cameras="{ laptop: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}, phone: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}}" \ # POLICY: 정책이 기대하는 키와 일치하는 키로 프레임을 획득하는 데 사용되는 카메라
    --task="dummy" \ # POLICY: 정책을 실행할 작업 (`Fold my t-shirt`). `act`와 같은 모든 정책에 대해 반드시 정의되지는 않습니다.
    --policy_type=your_policy_type \ # POLICY: 실행할 정책 유형 (smolvla, act 등)
    --pretrained_name_or_path=user/model \ # POLICY: 실행할 체크포인트에 대한 서버의 모델 이름/경로 (예: lerobot/smolvla_base)
    --policy_device=mps \ # POLICY: 서버에서 정책을 실행할 디바이스
    --actions_per_chunk=50 \ # POLICY: 한 번에 출력할 액션 수
    --chunk_size_threshold=0.5 \ # CLIENT: 서버에 새 관찰을 보내기 전 청크 크기의 임계값
    --aggregate_fn_name=weighted_average \ # CLIENT: 겹치는 부분에서 액션을 집계하는 함수
    --debug_visualize_queue_size=True # CLIENT: 런타임에 큐 크기를 시각화할지 여부
```
</hfoption>
<hfoption id="API example">

<!-- prettier-ignore-start -->
```python
import threading
from lerobot.robots.so100_follower import SO100FollowerConfig
from lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig
from lerobot.async_inference.configs import RobotClientConfig
from lerobot.async_inference.robot_client import RobotClient
from lerobot.async_inference.helpers import visualize_action_queue_size

# 1. 로봇 인스턴스 생성
"""시스템에서 사용 가능한 카메라를 확인하려면 `python lerobot/find_cameras.py`를 실행하세요"""
# 이 카메라는 정책이 기대하는 것과 일치해야 합니다
# 사용 중인 정책의 Hub에서 config.json을 확인하세요
camera_cfg = {
    "top": OpenCVCameraConfig(index_or_path=0, width=640, height=480, fps=30),
    "side": OpenCVCameraConfig(index_or_path=1, width=640, height=480, fps=30)
}

robot_cfg = SO100FollowerConfig(
  port="/dev/tty.usbmodem585A0076841",
  id="follower_so100",
  cameras=camera_cfg
)

# 3. 클라이언트 구성 생성
client_cfg = RobotClientConfig(
    robot=robot_cfg,
    server_address="localhost:8080",
    policy_device="mps",
    policy_type="smolvla",
    pretrained_name_or_path="fracapuano/smolvla_async",
    chunk_size_threshold=0.5,
    actions_per_chunk=50,  # 정책의 최대 액션보다 적은지 확인하세요
)

# 4. 클라이언트 생성 및 시작
client = RobotClient(client_cfg)

# 5. 작업 지정
task = "Don't do anything, stay still"

if client.start():
    # 액션 수신자 스레드 시작
    action_receiver_thread = threading.Thread(target=client.receive_actions, daemon=True)
    action_receiver_thread.start()

    try:
        # 제어 루프 실행
        client.control_loop(task)
    except KeyboardInterrupt:
        client.stop()
        action_receiver_thread.join()
        # (선택 사항) 액션 큐 크기 플롯
        visualize_action_queue_size(client.action_queue_size)
```
<!-- prettier-ignore-end -->

</hfoption>
</hfoptions>

다음 두 매개변수는 모든 설정에서 핵심입니다:

<table>
  <thead>
    <tr>
      <th>하이퍼파라미터</th>
      <th>기본값</th>
      <th>역할</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <code>actions_per_chunk</code>
      </td>
      <td>50</td>
      <td>
        정책이 한 번에 출력하는 액션 수. 일반적인 값: 10-50.
      </td>
    </tr>
    <tr>
      <td>
        <code>chunk_size_threshold</code>
      </td>
      <td>0.7</td>
      <td>
        큐가 ≤ 50% 찼을 때 클라이언트가 새로운 관찰을 보냅니다.
        값은 [0, 1]입니다.
      </td>
    </tr>
  </tbody>
</table>

<Tip>
  `actions_per_chunk`와 `chunk_size_threshold`의 다른 값은 다른 동작을 초래합니다.
</Tip>

한편으로는 `actions_per_chunk` 값을 늘리면 새 청크가 계산될 때 더 많은 액션을 사용할 수 있으므로 실행할 액션이 없는 상황이 발생할 가능성이 줄어듭니다.
그러나 `actions_per_chunk`의 더 큰 값은 더 긴 시간 범위에 걸쳐 액션을 예측하는 결과로 인한 복합 오류로 인해 덜 정확한 액션을 초래할 수도 있습니다.

반면에 `chunk_size_threshold` 값을 늘리면 `PolicyServer`에 추론을 위한 관찰을 더 자주 보내게 되어 상당한 부분에서 겹치는 업데이트 액션 청크가 더 많아집니다. 이는 높은 적응성을 초래하며, 극한에서는 각 관찰에 대해 하나의 액션 청크를 예측하고, 이는 새 청크가 생성되는 동안 약간만 소비됩니다.
이 옵션은 많은 요청의 결과로 추론 파이프라인에 더 많은 압력을 가합니다. 반대로 0.0에 가까운 `chunk_size_threshold` 값은 현재 청크가 소진될 때만 새 관찰이 전송되는 동기 엣지 케이스로 축소됩니다.

우리는 `actions_per_chunk`와 `chunk_size_threshold`의 기본값이 [SmolVLA 논문](https://huggingface.co/papers/2506.01844)을 위해 개발한 실험에서 잘 작동하는 것을 발견했지만, 설정에 가장 적합한 값을 찾기 위해 다른 값을 실험해 볼 것을 권장합니다.

### 설정에 맞게 비동기 추론 조정

1. **계산 리소스를 신중하게 선택하세요.** [PI0](https://huggingface.co/lerobot/pi0)은 추론 시 14GB의 메모리를 차지하는 반면, [SmolVLA](https://huggingface.co/lerobot/smolvla_base)는 약 2GB만 필요합니다. 더 작은 정책은 더 적은 계산 리소스가 필요하다는 점을 염두에 두고 사용 사례에 가장 적합한 계산 리소스를 식별해야 합니다. 사용된 정책과 디바이스의 조합(CPU 집약적, MPS 사용 또는 주어진 NVIDIA GPU의 CUDA 코어 수)은 예상해야 하는 평균 추론 지연 시간에 직접적인 영향을 미칩니다.
2. **추론 지연 시간을 기반으로 `fps`를 조정하세요.** 서버가 새 액션 청크를 생성하는 동안 클라이언트는 유휴 상태가 아니며 현재 액션 큐를 단계별로 진행합니다. 두 프로세스가 근본적으로 다른 속도로 발생하면 클라이언트가 빈 큐로 끝날 수 있습니다. 따라서 큐의 액션이 지속적으로 부족하면 fps를 줄여야 합니다.
3. **`chunk_size_threshold` 조정**.
   - `0.0`에 가까운 값은 거의 순차적인 동작을 초래합니다. `1.0`에 가까운 값 → 매 스텝마다 관찰 전송(더 많은 대역폭, 좋은 세계 모델에 의존).
   - 우리는 약 0.5-0.6의 값이 잘 작동하는 것을 발견했습니다. 이를 조정하려면 `--debug-visualize-queue-size`를 `True`로 설정하여 `RobotClient`를 시작하세요. 이렇게 하면 런타임에 액션 큐 크기 진화가 플롯되며, 설정에 가장 적합한 `chunk_size_threshold` 값을 찾는 데 사용할 수 있습니다.

<p align="center">
  <img
    src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/queues.png"
    width="80%"
  ></img>
</p>
<p align="center">
  <i>
    `--debug-visualize-queue-size` 플래그가 전달되면 다양한 수준의 `chunk_size_threshold`(SmolVLA 논문의 `g`)에 대해 런타임에 액션 큐 크기가 플롯됩니다.
  </i>
</p>

---

## 결론

비동기 추론은 실시간 로봇 제어의 중요한 발전을 나타내며, 로봇 공학 애플리케이션을 오랫동안 괴롭혀온 추론 지연의 근본적인 문제를 해결합니다. 이 튜토리얼을 통해 유휴 프레임을 제거하고 더 부드럽고 반응적인 로봇 동작을 가능하게 하는 완전한 비동기 추론 파이프라인을 구현하는 방법을 배웠습니다.

**주요 요점:**

- **패러다임 전환**: 비동기 추론은 액션 예측과 실행을 분리하여 로봇이 새 액션 청크가 병렬로 계산되는 동안 계속 행동할 수 있게 합니다
- **성능 이점**: 동기 접근 방식에 내재된 "추론 대기" 지연을 제거하며, 정책 모델이 커질수록 점점 더 중요해집니다
- **유연한 아키텍처**: 서버-클라이언트 설계는 분산 컴퓨팅을 가능하게 하여 실시간 로봇 제어를 유지하면서 강력한 원격 하드웨어에서 추론을 실행할 수 있습니다
- **조정 가능한 매개변수**: 성공은 특정 하드웨어, 정책 및 작업 요구 사항에 맞게 `actions_per_chunk`와 `chunk_size_threshold`를 적절하게 구성하는 데 달려 있습니다
- **범용 호환성**: 경량 ACT 모델에서 SmolVLA와 같은 비전-언어 모델까지 모든 LeRobot 지원 정책과 작동합니다

기본 매개변수로 실험을 시작하고, 액션 큐 크기를 모니터링하고, 특정 사용 사례에 최적의 성능을 달성하기 위해 설정을 반복적으로 개선하세요.
이에 대해 더 논의하고 싶다면 [Discord 커뮤니티](https://discord.gg/s3KuuzsPFb)로 오거나 [GitHub 저장소](https://github.com/lerobot/lerobot/issues)에 이슈를 여세요.
