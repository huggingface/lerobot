# 프로세서 파이프라인 디버그

프로세서 파이프라인은 여러 변환 단계를 연결할 때 특히 복잡할 수 있습니다.
단순한 함수 호출과 달리 파이프라인은 자연스러운 관찰 가능성이 부족하여 각 단계 사이에 무슨 일이 일어나는지 또는 어디서 잘못되는지 쉽게 볼 수 없습니다.
이 가이드는 이러한 문제를 해결하고 파이프라인을 통한 데이터 흐름을 이해하는 데 특별히 설계된 디버깅 도구와 기술을 제공합니다.

런타임 모니터링을 위한 **훅**, 세부 검사를 위한 **단계별 디버깅**, 구조적 불일치를 포착하기 위한 **기능 검증**의 세 가지 보완적인 디버깅 접근 방식을 탐구합니다. 각각은 다른 목적을 제공하며 함께 파이프라인 동작에 대한 완전한 가시성을 제공합니다.

## 훅 이해하기

훅은 파이프라인 실행 중 특정 시점에 호출되는 함수입니다.
파이프라인 코드를 변경하지 않고 데이터를 검사, 모니터링 또는 수정하는 방법을 제공합니다.
파이프라인에 대한 "이벤트 리스너"로 생각하세요.

### 훅이란 무엇인가?

훅은 파이프라인 실행 중 특정 순간에 자동으로 호출되는 콜백 함수입니다.
이 개념은 이벤트 기반 프로그래밍에서 나온 것으로, 파이프라인의 실행 흐름에 "훅을 걸어" 무슨 일이 일어나는지 관찰하거나 반응할 수 있다고 상상해보세요.

훅을 파이프라인에 체크포인트를 삽입하는 것으로 생각하세요. 파이프라인이 이러한 체크포인트 중 하나에 도달할 때마다 잠시 멈춰서 훅 함수를 호출하여 현재 상태를 검사하고, 정보를 기록하고, 데이터를 검증할 기회를 제공합니다.

훅은 두 개의 매개변수를 받는 간단한 함수입니다:

- `step_idx: int` - 현재 처리 단계의 인덱스 (0, 1, 2 등)
- `transition: EnvTransition` - 파이프라인의 해당 시점에서의 데이터 전환

훅의 아름다움은 비침습적 특성에 있습니다: 파이프라인 코드의 한 줄도 변경하지 않고 모니터링, 검증 또는 디버깅 로직을 추가할 수 있습니다. 파이프라인은 핵심 로직에 집중하고 깨끗하게 유지되며, 훅은 로깅, 모니터링, 디버깅과 같은 횡단 관심사를 처리합니다.

### Before vs After 훅

파이프라인은 두 가지 유형의 훅을 지원합니다:

- **Before 훅** (`register_before_step_hook`) - 각 단계가 실행되기 전에 호출됨
- **After 훅** (`register_after_step_hook`) - 각 단계가 완료된 후 호출됨

```python
def before_hook(step_idx: int, transition: EnvTransition):
    """단계가 전환을 처리하기 전에 호출됩니다."""
    print(f"About to execute step {step_idx}")
    # 유용한 용도: 로깅, 검증, 설정

def after_hook(step_idx: int, transition: EnvTransition):
    """단계가 전환을 처리한 후 호출됩니다."""
    print(f"Completed step {step_idx}")
    # 유용한 용도: 결과 모니터링, 정리, 디버깅

processor.register_before_step_hook(before_hook)
processor.register_after_step_hook(after_hook)
```

### NaN 감지 훅 구현

다음은 NaN 값을 감지하는 훅의 실용적인 예제입니다:

```python
def check_nans(step_idx: int, transition: EnvTransition):
    """관찰에서 NaN 값을 확인합니다."""
    obs = transition.get(TransitionKey.OBSERVATION)
    if obs:
        for key, value in obs.items():
            if isinstance(value, torch.Tensor) and torch.isnan(value).any():
                print(f"NaN detected in {key} at step {step_idx}")

# 각 단계 후에 실행되도록 훅을 등록
processor.register_after_step_hook(check_nans)

# 데이터 처리 - 훅이 자동으로 호출됨
output = processor(input_data)

# 디버깅이 완료되면 훅 제거
processor.unregister_after_step_hook(check_nans)
```

### 훅의 내부 작동 방식

내부 메커니즘을 이해하면 훅을 더 효과적으로 사용할 수 있습니다. 파이프라인은 두 개의 별도 리스트를 유지합니다: 하나는 before-step 훅용이고 다른 하나는 after-step 훅용입니다. 훅을 등록하면 적절한 리스트에 추가됩니다.

실행 중에 파이프라인은 엄격한 순서를 따릅니다: 각 처리 단계에 대해 먼저 모든 before-hooks를 등록 순서대로 호출한 다음 실제 단계 변환을 실행하고 마지막으로 모든 after-hooks를 등록 순서대로 호출합니다. 이것은 각 단계 주위에 예측 가능한 샌드위치 같은 구조를 만듭니다.

핵심 통찰은 훅이 핵심 파이프라인 로직을 변경하지 않는다는 것입니다—순전히 추가적입니다. 파이프라인의 `_forward` 메서드는 훅과 처리 단계 사이의 이 춤을 조율하여 디버깅 또는 모니터링 코드가 주요 데이터 흐름을 방해하지 않고 정확한 순간에 실행되도록 합니다.

다음은 파이프라인이 훅을 실행하는 방법의 간략한 보기입니다:

```python
class DataProcessorPipeline:
    def __init__(self):
        self.steps = [...]
        self.before_step_hooks = []  # Before 훅 리스트
        self.after_step_hooks = []   # After 훅 리스트

    def _forward(self, transition):
        """모든 단계를 통해 전환을 처리하는 내부 메서드."""
        for step_idx, processor_step in enumerate(self.steps):
            # 1. 모든 BEFORE 훅 호출
            for hook in self.before_step_hooks:
                hook(step_idx, transition)

            # 2. 실제 처리 단계 실행
            transition = processor_step(transition)

            # 3. 모든 AFTER 훅 호출
            for hook in self.after_step_hooks:
                hook(step_idx, transition)

        return transition

    def register_before_step_hook(self, hook_fn):
        self.before_step_hooks.append(hook_fn)

    def register_after_step_hook(self, hook_fn):
        self.after_step_hooks.append(hook_fn)
```

### 실행 흐름

실행 흐름은 다음과 같습니다:

```
Input → Before Hook → Step 0 → After Hook → Before Hook → Step 1 → After Hook → ... → Output
```

예를 들어 3개의 단계와 두 훅 유형이 있는 경우:

```python
def timing_before(step_idx, transition):
    print(f"⏱️  Starting step {step_idx}")

def validation_after(step_idx, transition):
    print(f"✅ Completed step {step_idx}")

processor.register_before_step_hook(timing_before)
processor.register_after_step_hook(validation_after)

# 다음을 출력합니다:
# ⏱️  Starting step 0
# ✅ Completed step 0
# ⏱️  Starting step 1
# ✅ Completed step 1
# ⏱️  Starting step 2
# ✅ Completed step 2
```

### 여러 훅

같은 유형의 여러 훅을 등록할 수 있습니다 - 등록된 순서대로 실행됩니다:

```python
def log_shapes(step_idx: int, transition: EnvTransition):
    obs = transition.get(TransitionKey.OBSERVATION)
    if obs:
        print(f"Step {step_idx} observation shapes:")
        for key, value in obs.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")

processor.register_after_step_hook(check_nans)      # 먼저 실행
processor.register_after_step_hook(log_shapes)     # 두 번째로 실행

# 두 훅 모두 각 단계 후에 등록 순서대로 호출됩니다
output = processor(input_data)
```

훅은 특정 문제를 모니터링하거나(예: NaN 감지) 일반 파이프라인 실행 중 메트릭을 수집하는 데 훌륭하지만 때로는 더 깊이 파고들어야 합니다. 각 단계에서 정확히 무슨 일이 일어나는지 이해하거나 복잡한 변환 로직을 디버그하고 싶을 때 단계별 디버깅은 필요한 세부 검사를 제공합니다.

## 단계별 디버깅

단계별 디버깅은 파이프라인에 대한 슬로우 모션 리플레이를 갖는 것과 같습니다. 입력에서 출력으로 데이터가 빠르게 변환되는 것을 한 번에 보는 대신 각 개별 단계 후에 무슨 일이 일어나는지 일시 정지하고 검사할 수 있습니다.

이 접근 방식은 복잡한 파이프라인을 이해하려고 할 때, 예상치 못한 동작을 디버그할 때 또는 각 변환이 예상대로 작동하는지 확인하려고 할 때 특히 유용합니다. 자동화된 모니터링에 적합한 훅과 달리 단계별 디버깅은 검사 프로세스에 대한 수동의 대화형 제어를 제공합니다.

`step_through()` 메서드는 각 처리 단계 후 전환 상태를 생성하는 제너레이터로, 중간 결과를 검사할 수 있습니다. 파이프라인을 통해 데이터가 흐를 때 일련의 스냅샷을 생성하는 것으로 생각하세요—각 스냅샷은 하나 더 많은 변환이 적용된 후 데이터가 정확히 어떻게 보이는지 보여줍니다.

### 단계별 작동 방식

`step_through()` 메서드는 파이프라인이 실행되는 방식을 근본적으로 변경합니다. 모든 단계를 순서대로 실행하고 최종 결과만 반환하는 대신 중간 결과를 생성하는 반복자로 파이프라인을 변환합니다.

내부적으로 무슨 일이 일어나는지는 다음과 같습니다: 이 메서드는 입력 데이터를 파이프라인의 내부 전환 형식으로 변환한 다음 이 초기 상태를 생성합니다. 다음으로 첫 번째 처리 단계를 적용하고 결과를 생성합니다. 그런 다음 두 번째 단계를 해당 결과에 적용하고 다시 생성하는 식입니다. 각 `yield`는 해당 시점의 전환에 대한 완전한 스냅샷을 제공합니다.

이 제너레이터 패턴은 게으르기 때문에 강력합니다—파이프라인은 요청할 때만 다음 단계를 계산합니다. 이는 언제든지 멈추고, 현재 상태를 철저히 검사하고, 계속할지 결정할 수 있음을 의미합니다. 하나의 문제 단계를 디버그하기 위해 전체 파이프라인을 실행해야 하는 것이 아닙니다.

전체 파이프라인을 실행하고 최종 결과만 보는 대신 `step_through()`는 각 단계 후에 일시 정지하고 중간 전환을 제공합니다:

```python
# 중간 상태를 생성하는 제너레이터를 생성합니다
for i, intermediate_result in enumerate(processor.step_through(input_data)):
    print(f"=== After step {i} ===")

    # 이 단계에서 관찰 검사
    obs = intermediate_result.get(TransitionKey.OBSERVATION)
    if obs:
        for key, value in obs.items():
            if isinstance(value, torch.Tensor):
                print(f"{key}: shape={value.shape}, dtype={value.dtype}")
```

### 중단점을 사용한 대화형 디버깅

단계별 루프에 중단점을 추가하여 대화형으로 디버그할 수 있습니다:

```python
# 디버깅과 함께 파이프라인 단계별 실행
for i, intermediate in enumerate(processor.step_through(data)):
    print(f"Step {i}: {processor.steps[i].__class__.__name__}")

    # 현재 상태를 검사하기 위해 중단점 설정
    breakpoint()  # 디버거가 여기서 일시 정지됩니다

    # 이제 디버거에서 'intermediate'를 검사할 수 있습니다:
    # - 텐서 모양과 값 확인
    # - 예상 변환 확인
    # - 예상치 못한 변경 사항 찾기
```

디버거 세션 중에 다음을 수행할 수 있습니다:

- `intermediate[TransitionKey.OBSERVATION]`을 검사하여 관찰 데이터 확인
- `intermediate[TransitionKey.ACTION]`을 확인하여 액션 변환 확인
- 전환의 모든 부분을 검사하여 각 단계가 수행하는 작업 이해

단계별 디버깅은 _데이터_ 변환을 이해하는 데 완벽하지만 해당 데이터의 _구조_는 어떻습니까? 훅과 단계별이 런타임 동작을 디버그하는 데 도움이 되지만 파이프라인이 다운스트림 구성 요소가 예상하는 형식의 데이터를 생성하는지도 확인해야 합니다. 이것이 기능 계약 검증이 필요한 곳입니다.

## 기능 계약 검증

기능 계약은 파이프라인이 입력으로 예상하고 출력으로 생성하는 데이터 구조를 정의합니다.
이러한 계약을 검증하면 불일치를 조기에 포착하는 데 도움이 됩니다.

### 기능 계약 이해

각 프로세서 단계에는 데이터 구조를 어떻게 변경하는지 설명하는 `transform_features()` 메서드가 있습니다:

```python
# 파이프라인에서 예상되는 출력 기능 가져오기
initial_features = {
    PipelineFeatureType.OBSERVATION: {
        "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(7,)),
        "observation.image": PolicyFeature(type=FeatureType.IMAGE, shape=(3, 224, 224))
    },
    PipelineFeatureType.ACTION: {
        "action": PolicyFeature(type=FeatureType.ACTION, shape=(4,))
    }
}

# 파이프라인이 출력할 내용 확인
output_features = processor.transform_features(initial_features)

print("Input features:")
for feature_type, features in initial_features.items():
    print(f"  {feature_type}:")
    for key, feature in features.items():
        print(f"    {key}: {feature.type.value}, shape={feature.shape}")

print("\nOutput features:")
for feature_type, features in output_features.items():
    print(f"  {feature_type}:")
    for key, feature in features.items():
        print(f"    {key}: {feature.type.value}, shape={feature.shape}")
```

### 예상 기능 확인

파이프라인이 예상하는 기능을 생성하는지 확인하세요:

```python
# 파이프라인이 생성할 것으로 예상하는 기능 정의
expected_keys = ["observation.state", "observation.image", "action"]

print("Validating feature contract...")
for expected_key in expected_keys:
    found = False
    for feature_type, features in output_features.items():
        if expected_key in features:
            feature = features[expected_key]
            print(f"✅ {expected_key}: {feature.type.value}, shape={feature.shape}")
            found = True
            break

    if not found:
        print(f"❌ Missing expected feature: {expected_key}")
```

이 검증은 파이프라인이 특정 데이터 구조를 예상하는 다운스트림 구성 요소와 올바르게 작동하는지 확인하는 데 도움이 됩니다.

## 요약

이제 세 가지 디버깅 접근 방식을 이해했으므로 모든 파이프라인 문제를 체계적으로 해결할 수 있습니다:

1. **훅** - 파이프라인 코드를 수정하지 않고 런타임 모니터링 및 검증을 위해
2. **단계별** - 중간 상태를 검사하고 변환을 이해하기 위해
3. **기능 검증** - 데이터 구조 계약이 충족되는지 확인하기 위해

**각 접근 방식을 사용하는 시기:**

- 파이프라인이 수행하는 작업을 이해해야 하거나 예상치 못한 일이 발생할 때 **단계별 디버깅**으로 시작하세요
- 개발 및 프로덕션 중 지속적인 모니터링을 위해 문제를 자동으로 포착하기 위해 **훅**을 추가하세요
- 배포 전에 **기능 검증**을 사용하여 파이프라인이 다운스트림 구성 요소와 작동하는지 확인하세요

이 세 가지 도구는 함께 작동하여 복잡한 파이프라인에 자연스럽게 부족한 완전한 관찰 가능성을 제공합니다. 문제를 감시하는 훅, 동작을 이해하는 데 도움이 되는 단계별, 호환성을 보장하는 기능 검증을 통해 모든 파이프라인을 자신 있고 효율적으로 디버그할 수 있습니다.
