#!/usr/bin/env python
"""
Comprehensive torch.compile benchmark for LeRobot policies.

Compares main branch performance with torch.compile optimized version.

Usage:
    python benchmark_compile_safe.py --policy act --device cuda
    python benchmark_compile_safe.py --policy diffusion --device cpu
"""

import argparse
import json
import time
from collections.abc import Callable
from typing import Any

import numpy as np
import torch
from torch.utils.data import DataLoader

from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata

# Safe imports for all processor functions
from lerobot.policies.act.processor_act import make_act_pre_post_processors
from lerobot.policies.diffusion.processor_diffusion import make_diffusion_pre_post_processors
from lerobot.policies.factory import make_policy, make_policy_config
from lerobot.policies.pi0.processor_pi0 import make_pi0_pre_post_processors
from lerobot.policies.smolvla.processor_smolvla import make_smolvla_pre_post_processors
from lerobot.policies.tdmpc.processor_tdmpc import make_tdmpc_pre_post_processors
from lerobot.policies.vqbet.processor_vqbet import make_vqbet_pre_post_processors


class PolicyConfig:
    """Type-safe policy configuration"""

    def __init__(
        self,
        processor_func: Callable,
        config_kwargs: dict[str, Any],
        delta_timestamps_func: Callable[[Any, float], dict[str, list[float]]],
    ):
        self.processor_func = processor_func
        self.config_kwargs = config_kwargs
        self.delta_timestamps_func = delta_timestamps_func


# Safe policy configuration mapping
POLICY_CONFIGS: dict[str, PolicyConfig] = {
    "act": PolicyConfig(
        processor_func=make_act_pre_post_processors,
        config_kwargs={"use_vae": False, "chunk_size": 100, "n_action_steps": 10},
        delta_timestamps_func=lambda cfg, fps: {"action": [i / fps for i in range(cfg.chunk_size)]},
    ),
    "diffusion": PolicyConfig(
        processor_func=make_diffusion_pre_post_processors,
        config_kwargs={"n_obs_steps": 2, "horizon": 16, "n_action_steps": 8},
        delta_timestamps_func=lambda cfg, fps: {"action": [i / fps for i in range(cfg.n_action_steps)]},
    ),
    "pi0": PolicyConfig(
        processor_func=make_pi0_pre_post_processors,
        config_kwargs={"n_obs_steps": 1, "chunk_size": 50, "n_action_steps": 50},
        delta_timestamps_func=lambda cfg, fps: {"action": [i / fps for i in range(cfg.chunk_size)]},
    ),
    "tdmpc": PolicyConfig(
        processor_func=make_tdmpc_pre_post_processors,
        config_kwargs={"horizon": 5, "n_action_steps": 1},
        delta_timestamps_func=lambda cfg, fps: {"action": [0]},
    ),
    "vqbet": PolicyConfig(
        processor_func=make_vqbet_pre_post_processors,
        config_kwargs={"n_obs_steps": 1, "chunk_size": 8, "n_action_steps": 8},
        delta_timestamps_func=lambda cfg, fps: {"action": [i / fps for i in range(cfg.chunk_size)]},
    ),
    "smolvla": PolicyConfig(
        processor_func=make_smolvla_pre_post_processors,
        config_kwargs={"n_obs_steps": 1, "chunk_size": 50, "n_action_steps": 50},
        delta_timestamps_func=lambda cfg, fps: {"action": [i / fps for i in range(cfg.chunk_size)]},
    ),
}


class TorchCompileBenchmark:
    """Safe torch.compile benchmark for LeRobot policies"""

    def __init__(self, policy_name: str, device: str, repo_id: str = "AdilZtn/grab_red_cube_test_25"):
        self.policy_name = policy_name
        self.device = torch.device(device)
        self.repo_id = repo_id

        # Benchmark parameters
        self.n_inference = 100
        self.n_training = 50
        self.batch_size = 8
        self.warmup_steps = 10
        self.tolerance = 1e-5

        print(f"ü§ñ Torch.compile Benchmark for {policy_name.upper()} Policy")
        print(f"Device: {device}")
        print(f"PyTorch version: {torch.__version__}")
        print(f"Dataset: {repo_id}")
        print("=" * 60)

    def setup_policy_and_data(self) -> tuple[Any, dict, Any, Any]:
        """Setup policy, data, and processors"""
        torch.manual_seed(42)
        np.random.seed(42)

        # Load dataset metadata
        ds_meta = LeRobotDatasetMetadata(self.repo_id)

        # Get policy configuration safely
        if self.policy_name not in POLICY_CONFIGS:
            available_policies = list(POLICY_CONFIGS.keys())
            raise ValueError(f"Policy '{self.policy_name}' not supported. Available: {available_policies}")

        policy_config = POLICY_CONFIGS[self.policy_name]

        # Create policy configuration
        cfg = make_policy_config(self.policy_name, device=str(self.device), **policy_config.config_kwargs)

        # Create policy
        policy = make_policy(cfg, ds_meta=ds_meta)
        policy.to(self.device)

        # Create processors using the safe function reference
        preprocessor, postprocessor = policy_config.processor_func(cfg, dataset_stats=ds_meta.stats)

        # Setup dataset with appropriate delta_timestamps
        delta_timestamps = policy_config.delta_timestamps_func(cfg, ds_meta.fps)
        dataset = LeRobotDataset(self.repo_id, episodes=[0], delta_timestamps=delta_timestamps)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, drop_last=True)

        # Get sample batch
        sample_batch = next(iter(dataloader))

        # Move to device
        for key in sample_batch:
            if isinstance(sample_batch[key], torch.Tensor):
                sample_batch[key] = sample_batch[key].to(self.device)

        # Preprocess
        sample_batch = preprocessor(sample_batch)

        return policy, sample_batch, preprocessor, postprocessor

    def benchmark_inference(self, policy, batch) -> tuple[float, torch.Tensor]:
        """Benchmark inference performance"""
        policy.eval()

        # Warmup
        with torch.no_grad():
            for _ in range(self.warmup_steps):
                _ = policy.select_action(batch)
                if hasattr(policy, "reset"):
                    policy.reset()

        # Benchmark
        if torch.cuda.is_available():
            torch.cuda.synchronize()

        start_time = time.perf_counter()

        with torch.no_grad():
            for _ in range(self.n_inference):
                action = policy.select_action(batch)
                if hasattr(policy, "reset"):
                    policy.reset()

        if torch.cuda.is_available():
            torch.cuda.synchronize()

        end_time = time.perf_counter()
        avg_time = (end_time - start_time) / self.n_inference * 1000  # ms

        return avg_time, action

    def benchmark_training(self, policy, batch) -> tuple[float, list[float], list[float]]:
        """Benchmark training performance"""
        policy.train()
        optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)

        # Warmup
        for _ in range(self.warmup_steps):
            optimizer.zero_grad()
            loss, _ = policy.forward(batch)
            loss.backward()
            optimizer.step()
            if hasattr(policy, "reset"):
                policy.reset()

        # Benchmark
        if torch.cuda.is_available():
            torch.cuda.synchronize()

        start_time = time.perf_counter()
        losses = []
        grad_norms = []

        for _ in range(self.n_training):
            optimizer.zero_grad()
            loss, loss_dict = policy.forward(batch)
            loss.backward()

            # Calculate gradient norm safely
            grad_tensors = [p.grad.detach() for p in policy.parameters() if p.grad is not None]
            if grad_tensors:
                total_norm = torch.norm(torch.stack([torch.norm(g) for g in grad_tensors]))
            else:
                total_norm = torch.tensor(0.0)

            optimizer.step()

            losses.append(loss.item())
            grad_norms.append(total_norm.item())

            if hasattr(policy, "reset"):
                policy.reset()

        if torch.cuda.is_available():
            torch.cuda.synchronize()

        end_time = time.perf_counter()
        avg_time = (end_time - start_time) / self.n_training * 1000  # ms

        return avg_time, losses, grad_norms

    def test_correctness(self, policy_original, policy_compiled, batch) -> dict[str, Any]:
        """Test numerical correctness between original and compiled versions"""
        print("üîç Testing correctness...")

        # Test inference
        policy_original.eval()
        policy_compiled.eval()

        with torch.no_grad():
            # Reset both policies to same state
            if hasattr(policy_original, "reset"):
                policy_original.reset()
            if hasattr(policy_compiled, "reset"):
                policy_compiled.reset()

            action_original = policy_original.select_action(batch)
            action_compiled = policy_compiled.select_action(batch)

        action_diff = torch.abs(action_original - action_compiled).max().item()
        inference_correct = action_diff < self.tolerance

        # Test training forward pass
        policy_original.train()
        policy_compiled.train()

        # Use same random state
        torch.manual_seed(42)
        loss_original, _ = policy_original.forward(batch)

        torch.manual_seed(42)
        loss_compiled, _ = policy_compiled.forward(batch)

        loss_diff = torch.abs(loss_original - loss_compiled).item()
        training_correct = loss_diff < self.tolerance

        return {
            "inference_correct": inference_correct,
            "training_correct": training_correct,
            "action_diff": action_diff,
            "loss_diff": loss_diff,
        }

    def generate_report(self, results: dict[str, Any]) -> str:
        """Generate comprehensive markdown report including failures"""

        # Handle cases where benchmarks might have failed
        inference_time_orig = results.get("time_original_inference", float("inf"))
        inference_time_comp = results.get("time_compiled_inference", float("inf"))
        training_time_orig = results.get("time_original_training", float("inf"))
        training_time_comp = results.get("time_compiled_training", float("inf"))

        speedup_inference = results.get("speedup_inference", 0.0)
        speedup_training = results.get("speedup_training", 0.0)

        compilation_successful = results.get("compilation_successful", False)
        correctness_passed = results.get("correctness_passed", False)
        inference_benchmarked = results.get("inference_benchmarked", False)
        training_benchmarked = results.get("training_benchmarked", False)

        report = f"""# Torch.compile Benchmark Report: {self.policy_name.upper()}

## Environment
- **Policy**: {self.policy_name}
- **Device**: {self.device}
- **PyTorch**: {torch.__version__}
- **Dataset**: {self.repo_id}
- **Batch Size**: {self.batch_size}
- **Benchmark Parameters**: {self.n_inference} inference runs, {self.n_training} training runs

## üîß Compilation Results
- **Status**: {"‚úÖ SUCCESS" if compilation_successful else "‚ùå FAILED"}"""

        if not compilation_successful:
            compilation_error = results.get("compilation_error", "Unknown error")
            report += f"""
- **Error**: `{compilation_error}`
- **Impact**: Policy will fall back to eager execution"""

        report += f"""

## üéØ Correctness Results
- **Status**: {"‚úÖ PASSED" if correctness_passed else "‚ùå FAILED"}
- **Inference**: {"PASSED" if results["correctness"]["inference_correct"] else "FAILED"}
- **Training**: {"PASSED" if results["correctness"]["training_correct"] else "FAILED"}

### Detailed Differences
- **Max Action Difference**: {results["correctness"]["action_diff"]:.2e} (threshold: {self.tolerance:.2e})
- **Loss Difference**: {results["correctness"]["loss_diff"]:.2e} (threshold: {self.tolerance:.2e})"""

        if not correctness_passed:
            action_diff = results["correctness"]["action_diff"]
            loss_diff = results["correctness"]["loss_diff"]

            report += f"""

### ‚ö†Ô∏è Correctness Analysis
- **Action diff magnitude**: {action_diff:.2e} ({"SEVERE" if action_diff > 1e-3 else "MODERATE" if action_diff > 1e-4 else "MINOR"})
- **Loss diff magnitude**: {loss_diff:.2e} ({"SEVERE" if loss_diff > 1e-3 else "MODERATE" if loss_diff > 1e-4 else "MINOR"})
- **Likely causes**: Graph breaks, dynamic shapes, numerical precision issues"""

        report += """

## ‚ö° Performance Results

### Inference Performance"""

        if inference_benchmarked:
            report += f"""
- **Original**: {inference_time_orig:.2f} ms/iter
- **Compiled**: {inference_time_comp:.2f} ms/iter
- **üöÄ Speedup**: {speedup_inference:.2f}x"""

            if speedup_inference < 1.0:
                report += " (‚ö†Ô∏è SLOWDOWN)"
            elif speedup_inference < 1.1:
                report += " (‚ö†Ô∏è INSUFFICIENT)"
        else:
            report += """
- **Status**: ‚ùå Benchmark failed
- **Reason**: Could not complete inference timing"""

        report += """

### Training Performance"""

        if training_benchmarked:
            report += f"""
- **Original**: {training_time_orig:.2f} ms/iter
- **Compiled**: {training_time_comp:.2f} ms/iter
- **üöÄ Speedup**: {speedup_training:.2f}x"""

            if speedup_training < 1.0:
                report += " (‚ö†Ô∏è SLOWDOWN)"
            elif speedup_training < 1.1:
                report += " (‚ö†Ô∏è INSUFFICIENT)"
        else:
            report += """
- **Status**: ‚ùå Benchmark failed
- **Reason**: Could not complete training timing"""

        # Consistency metrics if available
        loss_consistency = results.get("loss_consistency", float("inf"))
        grad_norm_consistency = results.get("grad_norm_consistency", float("inf"))

        if loss_consistency != float("inf") and grad_norm_consistency != float("inf"):
            report += f"""

### Consistency Metrics
- **Average Loss Difference**: {loss_consistency:.2e}
- **Average Grad Norm Difference**: {grad_norm_consistency:.2e}"""

        report += f"""

## üìã Success Criteria Analysis
- **‚úÖ Compilation**: {"PASSED" if compilation_successful else "FAILED"}
- **‚úÖ Correctness**: {"PASSED" if correctness_passed else "FAILED"}
- **‚úÖ Performance**: {"PASSED" if speedup_inference > 1.1 and speedup_training > 1.1 else "FAILED"}
- **‚úÖ Benchmarking**: {"PASSED" if inference_benchmarked and training_benchmarked else "FAILED"}

## üéØ Overall Result
{"üéØ SUCCESS: Policy is torch.compile compatible!" if results["success"] else "‚ùå NEEDS WORK: torch.compile not yet functional"}

## üõ†Ô∏è Next Steps"""

        if not compilation_successful:
            report += """
1. **Fix compilation errors** - Enable torch._dynamo verbose mode for details
2. **Identify graph breaks** - Look for .item(), dynamic shapes, control flow
3. **Test incrementally** - Fix one issue at a time"""
        elif not correctness_passed:
            report += """
1. **Debug numerical differences** - Check for precision issues
2. **Verify tensor operations** - Ensure deterministic behavior
3. **Test with smaller tolerance** - May be acceptable for some use cases"""
        elif not (speedup_inference > 1.1 and speedup_training > 1.1):
            report += """
1. **Profile bottlenecks** - Use torch.profiler to identify slow ops
2. **Optimize compilation mode** - Try 'reduce-overhead' or 'max-autotune' modes
3. **Check graph breaks** - Even successful compilation can have breaks"""
        else:
            report += """
1. **Document changes** - Create clear PR with benchmark results
2. **Add tests** - Ensure torch.compile compatibility is maintained
3. **Consider edge cases** - Test with different batch sizes, VAE modes, etc."""

        report += f"""

## üîç Raw Data
```json
{json.dumps(results, indent=2)}
```
"""
        return report

    def run_benchmark(self) -> dict[str, Any]:
        """Run complete benchmark suite"""

        # Setup
        print("üì¶ Setting up policy and data...")
        try:
            policy, batch, preprocessor, postprocessor = self.setup_policy_and_data()
        except Exception as e:
            return {"success": False, "error": f"Setup failed: {str(e)}"}

        # Test compilation
        print("üîß Testing torch.compile...")
        compilation_error = None
        policy_compiled = None

        try:
            # Enable verbose compilation debugging
            import torch._dynamo

            torch._dynamo.config.verbose = True
            torch._dynamo.config.suppress_errors = False

            policy_compiled = torch.compile(policy, mode="default")

            # Force compilation by running once
            policy_compiled.eval()
            with torch.no_grad():
                _ = policy_compiled.select_action(batch)

            print("‚úÖ Compilation successful!")

        except Exception as e:
            compilation_error = str(e)
            print(f"‚ùå Compilation failed: {e}")
            # Continue with analysis even if compilation fails
            policy_compiled = policy  # Use original policy for comparison

        # Test correctness (always run this for analysis)
        print("üîç Testing correctness...")
        correctness = self.test_correctness(policy, policy_compiled, batch)

        # Print detailed correctness results
        print(f"   Inference correct: {correctness['inference_correct']}")
        print(f"   Training correct: {correctness['training_correct']}")
        print(f"   Max action difference: {correctness['action_diff']:.2e}")
        print(f"   Loss difference: {correctness['loss_diff']:.2e}")

        correctness_passed = correctness["inference_correct"] and correctness["training_correct"]
        if correctness_passed:
            print("‚úÖ Correctness tests passed!")
        else:
            print("‚ö†Ô∏è  Correctness test failed - continuing with timing analysis...")

        # Always run timing benchmarks for analysis
        print(f"üöÄ Benchmarking inference ({self.n_inference} runs)...")
        try:
            time_orig_inf, _ = self.benchmark_inference(policy, batch)
            time_comp_inf, _ = self.benchmark_inference(policy_compiled, batch)
            inference_benchmarked = True
        except Exception as e:
            print(f"‚ùå Inference benchmark failed: {e}")
            time_orig_inf = time_comp_inf = float("inf")
            inference_benchmarked = False

        print(f"üöÄ Benchmarking training ({self.n_training} runs)...")
        try:
            time_orig_train, losses_orig, grad_norms_orig = self.benchmark_training(policy, batch)
            time_comp_train, losses_comp, grad_norms_comp = self.benchmark_training(policy_compiled, batch)
            training_benchmarked = True

            loss_consistency = (
                np.mean(np.abs(np.array(losses_orig) - np.array(losses_comp)))
                if losses_orig and losses_comp
                else float("inf")
            )
            grad_norm_consistency = (
                np.mean(np.abs(np.array(grad_norms_orig) - np.array(grad_norms_comp)))
                if grad_norms_orig and grad_norms_comp
                else float("inf")
            )
        except Exception as e:
            print(f"‚ùå Training benchmark failed: {e}")
            time_orig_train = time_comp_train = float("inf")
            loss_consistency = grad_norm_consistency = float("inf")
            training_benchmarked = False

        # Calculate speedups (handle inf cases)
        speedup_inference = (
            time_orig_inf / time_comp_inf if inference_benchmarked and time_comp_inf > 0 else 0.0
        )
        speedup_training = (
            time_orig_train / time_comp_train if training_benchmarked and time_comp_train > 0 else 0.0
        )

        # Success criteria
        compilation_successful = compilation_error is None
        performance_good = speedup_inference > 1.1 and speedup_training > 1.1

        success = (
            compilation_successful
            and correctness_passed
            and performance_good
            and inference_benchmarked
            and training_benchmarked
        )

        results = {
            "success": success,
            "policy": self.policy_name,
            "device": str(self.device),
            "pytorch_version": torch.__version__,
            # Compilation results
            "compilation_successful": compilation_successful,
            "compilation_error": compilation_error,
            # Correctness results
            "correctness": correctness,
            "correctness_passed": correctness_passed,
            # Timing results
            "inference_benchmarked": inference_benchmarked,
            "training_benchmarked": training_benchmarked,
            "time_original_inference": time_orig_inf,
            "time_compiled_inference": time_comp_inf,
            "speedup_inference": speedup_inference,
            "time_original_training": time_orig_train,
            "time_compiled_training": time_comp_train,
            "speedup_training": speedup_training,
            # Consistency metrics
            "loss_consistency": loss_consistency,
            "grad_norm_consistency": grad_norm_consistency,
        }

        # Print detailed summary
        print("\nüìä DETAILED RESULTS SUMMARY")
        print("=" * 60)
        print(f"üîß Compilation: {'‚úÖ SUCCESS' if compilation_successful else '‚ùå FAILED'}")
        if compilation_error:
            print(f"   Error: {compilation_error}")

        print(f"üéØ Correctness: {'‚úÖ PASSED' if correctness_passed else '‚ùå FAILED'}")
        print(f"   Inference diff: {correctness['action_diff']:.2e} (threshold: {self.tolerance:.2e})")
        print(f"   Training diff:  {correctness['loss_diff']:.2e} (threshold: {self.tolerance:.2e})")

        print("‚ö° Performance:")
        if inference_benchmarked:
            print(f"   Inference: {time_orig_inf:.2f}ms ‚Üí {time_comp_inf:.2f}ms ({speedup_inference:.2f}x)")
        else:
            print("   Inference: ‚ùå Benchmark failed")

        if training_benchmarked:
            print(
                f"   Training:  {time_orig_train:.2f}ms ‚Üí {time_comp_train:.2f}ms ({speedup_training:.2f}x)"
            )
        else:
            print("   Training:  ‚ùå Benchmark failed")

        print(f"üèÜ Overall: {'‚úÖ SUCCESS' if success else '‚ùå NEEDS WORK'}")

        return results


def main():
    parser = argparse.ArgumentParser(description="Torch.compile benchmark for LeRobot policies")
    parser.add_argument(
        "--policy", required=True, choices=list(POLICY_CONFIGS.keys()), help="Policy to benchmark"
    )
    parser.add_argument(
        "--device", default="cuda" if torch.cuda.is_available() else "cpu", help="Device to run on"
    )
    parser.add_argument("--output", help="Output file for results (optional)")
    parser.add_argument("--n-inference", type=int, default=100, help="Number of inference runs")
    parser.add_argument("--n-training", type=int, default=50, help="Number of training runs")
    parser.add_argument("--batch-size", type=int, default=8, help="Batch size to use")

    args = parser.parse_args()

    # Run benchmark
    benchmark = TorchCompileBenchmark(args.policy, args.device)

    # Override default parameters if provided
    if args.n_inference:
        benchmark.n_inference = args.n_inference
    if args.n_training:
        benchmark.n_training = args.n_training
    if args.batch_size:
        benchmark.batch_size = args.batch_size

    results = benchmark.run_benchmark()

    # Always generate a report (even for failures)
    if "error" in results and len(results) == 2:  # Only basic error info
        # Simple failure case - create basic report
        simple_report = f"""# Benchmark Failed: {benchmark.policy_name.upper()}

## Error
{results.get("error", "Unknown error")}

## Environment
- **Policy**: {benchmark.policy_name}
- **Device**: {benchmark.device}
- **PyTorch**: {torch.__version__}

## Next Steps
1. Check the error message above
2. Ensure the policy and dataset are properly configured
3. Try with a simpler configuration first
"""
        if args.output:
            with open(args.output, "w") as f:
                f.write(simple_report)
            print(f"\nüíæ Basic error report saved to {args.output}")
        else:
            print("\nüìù BASIC ERROR REPORT:")
            print(simple_report)
    else:
        # Full benchmark was attempted - generate comprehensive report
        report = benchmark.generate_report(results)

        if args.output:
            with open(args.output, "w") as f:
                f.write(report)
            print(f"\nüíæ Comprehensive report saved to {args.output}")
        else:
            print("\nüìù COMPREHENSIVE REPORT:")
            print(report)


if __name__ == "__main__":
    main()
