#!/usr/bin/env python

# Copyright 2024 Exponential Deep Space Inc, Microsoft Inc,
# and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from collections import deque
from typing import Callable, List

import einops
import numpy as np
import torch
import torch.nn.functional as F
import torchvision
from huggingface_hub import PyTorchModelHubMixin
from torch import Tensor, nn

from transformers import AutoProcessor, AutoModelForCausalLM

from lerobot.common.policies.florence.configuration_florence import FlorenceConfig
from lerobot.common.policies.normalize import Normalize, Unnormalize
from lerobot.common.policies.utils import populate_queues

class Florence(nn.Module, PyTorchModelHubMixin):
    """
    Florence uses Microsoft's Florence2 VLM as the backbone and connects it with linear or diffusion action heads.
    Florence2 is heavily pretrained with 900M images, and supports VQA, OCR, object detection and segmentation tasks.
    Thus, it can be a useful prerained network for robotics policy. Compared with OpenVLA/RT-2, it's smaller with 0.2/0.7B params.
    For details: https://github.com/EDiRobotics/mimictest
    """

    name = "florence"

    def __init__(
        self,
        config: FlorenceConfig | None = None,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__()
        if config is None:
            config = FlorenceConfig()
        self.config = config
        self.normalize_inputs = Normalize(
            config.input_shapes, config.input_normalization_modes, dataset_stats
        )
        self.normalize_targets = Normalize(
            config.output_shapes, config.output_normalization_modes, dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_shapes, config.output_normalization_modes, dataset_stats
        )

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None

        self.model = FlorenceLinearWrapper(config)

        self.expected_image_keys = [k for k in config.input_shapes if k.startswith("observation.image")]
        self.use_robot_state = "observation.state" in config.input_shapes
        self.use_env_state = "observation.environment_state" in config.input_shapes

        self.reset()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if len(self.expected_image_keys) > 0:
            self._queues["observation.images"] = deque(maxlen=self.config.n_obs_steps)
        if self.use_robot_state:
            self._queues["observation.state"] = deque(maxlen=self.config.n_obs_steps)
        if self.use_env_state:
            self._queues["observation.environment_state"] = deque(maxlen=self.config.n_obs_steps)

    @torch.no_grad
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The florence policy model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """

        self.eval()

        batch = self.normalize_inputs(batch)
        if len(self.expected_image_keys) > 0:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues["action"]) == 0:
            # stack n latest observations from the queue
            batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
            actions = self.model(batch)

            # Extract `n_action_steps` steps worth of actions (from the current observation).
            start = self.config.n_obs_steps - 1
            end = start + self.config.n_action_steps
            actions = actions[:, start:end]

            # TODO(rcadene): make above methods return output dictionary?
            actions = self.unnormalize_outputs({"action": actions})["action"]

            self._queues["action"].extend(actions.transpose(0, 1))

        action = self._queues["action"].popleft()
        return action

    def forward(self, batch: dict[str, Tensor]) -> dict[str, Tensor]:
        """Run the batch through the model and compute the loss for training or validation."""
        batch = self.normalize_inputs(batch)
        batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
        batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
        batch = self.normalize_targets(batch)

        actions_hat = self.model(batch)
        l1_loss = (
            F.l1_loss(batch["action"], actions_hat, reduction="none") * ~batch["action_is_pad"].unsqueeze(-1)
        ).mean()

        return {
            "loss": l1_loss,
        }

class FlorenceLinearWrapper(nn.Module):
    """A wrapper of the Florence2 VLM and a linear action head
    
    TODO: the introduction
    """

    def __init__(self, config: FlorenceConfig):
        super().__init__()

        self.use_images = any(k.startswith("observation.image") for k in config.input_shapes)
        self.use_robot_state = "observation.state" in config.input_shapes
        self.use_env_state = "observation.environment_state" in config.input_shapes
        if self.use_images == False:
            raise Exception("modeling_florence_policy.py: you should use florence policy with image input")
        if config.crop_shape is not None:
            self.do_crop = True
            # Always use center crop for eval
            self.center_crop = torchvision.transforms.CenterCrop(config.crop_shape)
            if config.crop_is_random:
                self.maybe_random_crop = torchvision.transforms.RandomCrop(config.crop_shape)
            else:
                self.maybe_random_crop = self.center_crop
        else:
            self.do_crop = False

        self.florence = AutoModelForCausalLM.from_pretrained(config.repo_path, trust_remote_code=True)
        if config.freeze_vision_tower:
            for param in self.florence.vision_tower.parameters():
                param.requires_grad = False

        os.environ['TOKENIZERS_PARALLELISM'] = 'true'
        self.tokenizer = AutoProcessor.from_pretrained(config.repo_path, trust_remote_code=True).tokenizer
        prompt_token_ids = self.tokenizer(
            "<Action>",
            return_tensors="pt",
        )['input_ids']
        self.prompt_embeds = nn.Parameter(self.florence.get_input_embeddings()(prompt_token_ids), requires_grad=False)

        florence_dim = self.florence.language_model.model.decoder.embed_tokens.embedding_dim
        if self.use_robot_state:
            self.robot_state_encoder = nn.Linear(config.input_shapes["observation.state"][0], florence_dim)
        if self.use_env_state:
            self.env_state_encoder = nn.Linear(config.input_shapes["observation.environment_state"][0], florence_dim)

        self.horizon = config.horizon
        self.action_query = nn.Parameter(torch.zeros(1, self.horizon, florence_dim))
        action_dim = config.output_shapes["action"][0]
        self.to_action = nn.Linear(florence_dim, action_dim)

    def forward(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have (at least):
        {
            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
        }
        It can also contains:
        {
            "observation.state": (B, n_obs_steps, state_dim)
            "observation.environment_state": (B, environment_dim)
            "observation.text": A list contains B strings or a token id tensor (B, max_length)
            Note: to optimizer training speed and avoid CPU-GPU sync, "observation.text" should be a token id tensor on GPU
        }
        The output shape: (B, horizon, action_dim)
        """
        rgb = batch['observation.images']
        if self.do_crop:
            if self.training: 
                rgb = self.maybe_random_crop(rgb)
            else:
                # Always use center crop for eval.
                rgb = self.center_crop(rgb)
        B, T, V, C, H, W = rgb.shape
        rgb = rgb.view(B*T*V, C, H, W)
        rgb_features = self.florence._encode_image(rgb)
        B_T_V, N, D = rgb_features.shape
        rgb_features = rgb_features.view(B, T*V*N, D)

        text_embeds = self.prompt_embeds.repeat(B, 1, 1) # (b n d)
        if "observation.text" in batch:
            if isinstance(batch['observation.text'], list):
                text_token_ids = self.tokenizer(
                    batch['observation.text'],
                    return_tensors="pt",
                    padding='max_length',
                    max_length=self.max_num_text_tokens,
                )['input_ids'].to(rgb.device)
            else:
                text_token_ids = batch['observation.text']
            task_embeds = self.net.get_input_embeddings()(text_token_ids)
            text_embeds = torch.cat((text_embeds, task_embeds), dim=1)
        inputs_embeds, attention_mask = self.florence._merge_input_ids_with_image_features(rgb_features, text_embeds)

        decoder_inputs_embeds = self.action_query.repeat(B, 1, 1) # (b, horizon, d)
        if self.use_robot_state:
            robot_state = self.robot_state_encoder(batch["observation.state"]) # (b, t, d)
            decoder_inputs_embeds = torch.cat((robot_state, decoder_inputs_embeds), dim=1)
        if self.use_env_state:
            env_state = self.env_state_encoder(batch["observation.environment_state"]) # (b, t, d)
            decoder_inputs_embeds = torch.cat((env_state, decoder_inputs_embeds), dim=1)

        decoder_outputs_embeds = self.florence.language_model(
            inputs_embeds = inputs_embeds,
            decoder_inputs_embeds = decoder_inputs_embeds,
            output_hidden_states = True,
        )['decoder_hidden_states'][-1]
        pred_actions = self.to_action(decoder_outputs_embeds[:, -self.horizon:])
        return pred_actions 
