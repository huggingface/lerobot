# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Records a dataset. Actions for the robot can be either generated by teleoperation or by a policy.

Example:

```shell
lerobot-record \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58760431541 \
    --robot.cameras="{laptop: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}}" \
    --robot.id=black \
    --dataset.repo_id=<my_username>/<my_dataset_name> \
    --dataset.num_episodes=2 \
    --dataset.single_task="Grab the cube" \
    --display_data=true
    # <- Teleop optional if you want to teleoperate to record or in between episodes with a policy \
    # --teleop.type=so100_leader \
    # --teleop.port=/dev/tty.usbmodem58760431551 \
    # --teleop.id=blue \
    # <- Policy optional if you want to record with a policy \
    # --policy.path=${HF_USER}/my_policy \
```

Example recording with bimanual so100:
```shell
lerobot-record \
  --robot.type=bi_so100_follower \
  --robot.left_arm_port=/dev/tty.usbmodem5A460851411 \
  --robot.right_arm_port=/dev/tty.usbmodem5A460812391 \
  --robot.id=bimanual_follower \
  --robot.cameras='{
    left: {"type": "opencv", "index_or_path": 0, "width": 640, "height": 480, "fps": 30},
    top: {"type": "opencv", "index_or_path": 1, "width": 640, "height": 480, "fps": 30},
    right: {"type": "opencv", "index_or_path": 2, "width": 640, "height": 480, "fps": 30}
  }' \
  --teleop.type=bi_so100_leader \
  --teleop.left_arm_port=/dev/tty.usbmodem5A460828611 \
  --teleop.right_arm_port=/dev/tty.usbmodem5A460826981 \
  --teleop.id=bimanual_leader \
  --display_data=true \
  --dataset.repo_id=${HF_USER}/bimanual-so100-handover-cube \
  --dataset.num_episodes=25 \
  --dataset.single_task="Grab and handover the red cube to the other arm"
```
"""

import json
import logging
import sys
import time
from dataclasses import asdict, dataclass, field
from pathlib import Path
from pprint import pformat
from typing import Any

import cv2
import numpy as np
import torch

from lerobot.cameras import (  # noqa: F401
    CameraConfig,  # noqa: F401
)
from lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.configs import parser
from lerobot.configs.policies import PreTrainedConfig
from lerobot.datasets.image_writer import safe_stop_image_writer
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.pipeline_features import aggregate_pipeline_dataset_features, create_initial_features
from lerobot.datasets.utils import build_dataset_frame, combine_feature_dicts
from lerobot.datasets.video_utils import VideoEncodingManager
from lerobot.policies.attention_visualization import AttentionRecordingManager
from lerobot.policies.attention_visualization.recorder import (
    AttnVideoRecorder,
    _compose_side_by_side,
    _extract_images_from_obs_frame,
    _to_serializable,
)
from lerobot.policies.factory import make_policy, make_pre_post_processors
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.policies.utils import make_robot_action
from lerobot.processor import (
    PolicyAction,
    PolicyProcessorPipeline,
    RobotAction,
    RobotObservation,
    RobotProcessorPipeline,
    make_default_processors,
)
from lerobot.processor.rename_processor import rename_stats
from lerobot.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    bi_so100_follower,
    hope_jr,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    bi_so100_leader,
    homunculus,
    koch_leader,
    make_teleoperator_from_config,
    so100_leader,
    so101_leader,
)
from lerobot.teleoperators.keyboard.teleop_keyboard import KeyboardTeleop
from lerobot.utils.constants import ACTION, OBS_STR
from lerobot.utils.control_utils import (
    init_keyboard_listener,
    is_headless,
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.utils.import_utils import register_third_party_plugins
from lerobot.utils.robot_utils import precise_sleep
from lerobot.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.utils.visualization_utils import init_rerun, log_rerun_data

class AttnVideoRecorder:
    def __init__(self, output_path: Path, fps: int):
        self.output_path = Path(output_path)
        self.fps = fps
        self._writer = None
        self._open_failed = False
        # Discord などでも再生されやすいように H.264 系を優先し、駄目なら順にフォールバック
        self._codec_candidates = ["avc1", "H264", "mp4v", "MP4V", "XVID", "MJPG"]
        # ディレクトリだけは先に作成しておく（フレームが無くても存在が分かるように）
        self.output_path.parent.mkdir(parents=True, exist_ok=True)

    def _ensure_writer(self, frame_size: tuple[int, int]) -> bool:
        if self._writer is not None:
            return True
        if self._open_failed:
            return False

        w, h = frame_size
        self.output_path.parent.mkdir(parents=True, exist_ok=True)

        for codec in self._codec_candidates:
            fourcc = cv2.VideoWriter_fourcc(*codec)
            writer = cv2.VideoWriter(str(self.output_path), fourcc, self.fps, (w, h))
            if writer is not None and writer.isOpened():
                self._writer = writer
                logging.info("Attention video: using codec=%s path=%s", codec, self.output_path)
                return True
            if writer is not None:
                writer.release()

        logging.error("Attention video: failed to open writer for %s", self.output_path)
        self._open_failed = True
        return False

    def add_frame(self, frame_bgr: np.ndarray):
        # frame_bgr: H x W x 3, uint8 (BGR)
        h, w = frame_bgr.shape[:2]
        if not self._ensure_writer((w, h)):
            return
        self._writer.write(frame_bgr)

    def close(self):
        if self._writer is not None:
            self._writer.release()
            self._writer = None


def _extract_first_image_from_obs_frame(observation_frame: dict[str, Any]) -> np.ndarray | None:
    """
    build_dataset_frame で作られた observation_frame から、
    最初に見つかった 3次元配列を画像として返す。CHW なら HWC に変換。
    """
    for v in observation_frame.values():
        if isinstance(v, np.ndarray) and v.ndim == 3:
            img = v
            # CHW → HWC っぽい場合は transpose
            if img.shape[0] in (1, 3) and img.shape[-1] not in (1, 3):
                img = np.transpose(img, (1, 2, 0))
            img = np.ascontiguousarray(img)
            if img.dtype != np.uint8:
                img = np.clip(img * 255.0, 0, 255).astype(np.uint8)
            return img
    return None


def _extract_images_from_obs_frame(observation_frame: dict[str, Any]) -> dict[str, np.ndarray]:
    """
    observation_frame に含まれる全画像をキー付きで取得（CHW は HWC に直し、常に BGR に変換）。
    """
    out: dict[str, np.ndarray] = {}
    for k, v in observation_frame.items():
        if isinstance(v, np.ndarray) and v.ndim == 3:
            img = v
            if img.shape[0] in (1, 3) and img.shape[-1] not in (1, 3):
                img = np.transpose(img, (1, 2, 0))
            img = np.ascontiguousarray(img)
            if img.dtype != np.uint8:
                img = np.clip(img * 255.0, 0, 255).astype(np.uint8)
            # to BGR
            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
            out[k] = img_bgr
    return out


def _render_attn_overlay(
    img_bgr: np.ndarray,
    policy: PreTrainedPolicy,
    img_range_override: tuple[int, int] | None = None,
) -> tuple[np.ndarray | None, str | None]:
    """
    SmolVLAPolicy の内部に持たせた
      - policy.model.vlm_with_expert.last_attn  (B, heads, L_q, L_k)
      - policy.model.last_image_patch_range     (img_start, img_end)
    を使って、画像に attention ヒートマップを重ねた1フレームを返す。
    """
    debug_reason: str | None = None

    if not hasattr(policy, "model"):
        return None, "policy_has_no_model"
    model = policy.model
    if not hasattr(model, "vlm_with_expert"):
        return None, "model_has_no_vlm"
    vlm = model.vlm_with_expert

    attn = getattr(vlm, "last_attn", None)
    img_ranges = getattr(model, "last_image_patch_ranges", None)
    img_range_single = getattr(model, "last_image_patch_range", None)

    # まず指定された範囲を優先。無ければ複数カメラの先頭、さらに単一範囲でフォールバック。
    if img_range_override is not None:
        img_start, img_end = img_range_override
    elif img_ranges and len(img_ranges) > 0:
        img_start, img_end = img_ranges[0]
    elif img_range_single is not None:
        img_start, img_end = img_range_single
    else:
        return None, "attn_or_img_range_missing"

    if attn is None:
        return None, "attn_missing"

    # 期待形状: (B, heads, L_q, L_k)
    if not isinstance(attn, torch.Tensor) or attn.ndim != 4 or attn.shape[0] < 1:
        return None, "attn_bad_shape"

    # batch 0
    attn_b = attn[0]                     # [heads, L_q, L_k]
    attn_mean_heads = attn_b.mean(0)     # [L_q, L_k]

    # 最後の query トークンからの注目分布を見る（最後のアクションステップ）
    last_q = attn_mean_heads[-1]         # [L_k]
    if img_end > last_q.shape[-1]:
        return None, f"img_end_out_of_range({img_end}>{last_q.shape[-1]})"

    img_attn = last_q[img_start:img_end]  # [N_patches]
    n_patches = img_attn.shape[0]
    if n_patches <= 0:
        return None, "no_patches"

    # パッチ数から正方形グリッドを推定（例: 14x14=196 等）
    grid_size = int(round(float(n_patches) ** 0.5))
    if grid_size * grid_size != n_patches:
        # きれいな正方形でなければ今回は諦める
        return None, f"non_square_patch_count({n_patches})"

    attn_map = img_attn.detach().cpu().numpy().reshape(grid_size, grid_size)
    attn_map = attn_map - attn_map.min()
    maxv = attn_map.max()
    if maxv > 0:
        attn_map = attn_map / maxv

    h, w = img_bgr.shape[:2]
    attn_resized = cv2.resize(attn_map, (w, h), interpolation=cv2.INTER_LINEAR)
    attn_uint8 = (attn_resized * 255).astype(np.uint8)
    heatmap = cv2.applyColorMap(attn_uint8, cv2.COLORMAP_JET)

    overlay = cv2.addWeighted(img_bgr, 0.5, heatmap, 0.5, 0.0)
    return overlay, None

@dataclass
class DatasetRecordConfig:
    # Dataset identifier. By convention it should match '{hf_username}/{dataset_name}' (e.g. `lerobot/test`).
    repo_id: str
    # A short but accurate description of the task performed during the recording (e.g. "Pick the Lego block and drop it in the box on the right.")
    single_task: str
    # Root directory where the dataset will be stored (e.g. 'dataset/path').
    root: str | Path | None = None
    # Limit the frames per second.
    fps: int = 30
    # Number of seconds for data recording for each episode.
    episode_time_s: int | float = 60
    # Number of seconds for resetting the environment after each episode.
    reset_time_s: int | float = 60
    # Number of episodes to record.
    num_episodes: int = 50
    # Encode frames in the dataset into video
    video: bool = True
    # Upload dataset to Hugging Face hub.
    push_to_hub: bool = True
    # Upload on private repository on the Hugging Face hub.
    private: bool = False
    # Add tags to your dataset on the hub.
    tags: list[str] | None = None
    # Number of subprocesses handling the saving of frames as PNG. Set to 0 to use threads only;
    # set to ≥1 to use subprocesses, each using threads to write images. The best number of processes
    # and threads depends on your system. We recommend 4 threads per camera with 0 processes.
    # If fps is unstable, adjust the thread count. If still unstable, try using 1 or more subprocesses.
    num_image_writer_processes: int = 0
    # Number of threads writing the frames as png images on disk, per camera.
    # Too many threads might cause unstable teleoperation fps due to main thread being blocked.
    # Not enough threads might cause low camera fps.
    num_image_writer_threads_per_camera: int = 4
    # Number of episodes to record before batch encoding videos
    # Set to 1 for immediate encoding (default behavior), or higher for batched encoding
    video_encoding_batch_size: int = 1
    # Rename map for the observation to override the image and state keys
    rename_map: dict[str, str] = field(default_factory=dict)

    def __post_init__(self):
        if self.single_task is None:
            raise ValueError("You need to provide a task as argument in `single_task`.")


@dataclass
class RecordConfig:
    robot: RobotConfig
    dataset: DatasetRecordConfig
    # Whether to control the robot with a teleoperator
    teleop: TeleoperatorConfig | None = None
    # Whether to control the robot with a policy
    policy: PreTrainedConfig | None = None
    # Display all cameras on screen
    display_data: bool = False
    # Use vocal synthesis to read events.
    play_sounds: bool = True
    # Resume recording on an existing dataset.
    resume: bool = False

    def __post_init__(self):
        # HACK: We parse again the cli args here to get the pretrained path if there was one.
        policy_path = parser.get_path_arg("policy")
        if policy_path:
            cli_overrides = parser.get_cli_overrides("policy")
            self.policy = PreTrainedConfig.from_pretrained(policy_path, cli_overrides=cli_overrides)
            self.policy.pretrained_path = policy_path

        if self.teleop is None and self.policy is None:
            raise ValueError("Choose a policy, a teleoperator or both to control the robot")

    @classmethod
    def __get_path_fields__(cls) -> list[str]:
        """This enables the parser to load config from the policy using `--policy.path=local/dir`"""
        return ["policy"]


class SimpleRecordingManager:
    """
    Policyを使わない場合に、関節角ログと全カメラ結合動画を記録する簡易レコーダ。
    episode_values_{idx}.json と all_cameras_episode_{idx}.mp4 を attn_videos 配下に出力。
    """

    def __init__(self, output_root: Path, repo_id: str, fps: int):
        self.output_root = Path(output_root)
        self.output_root.mkdir(parents=True, exist_ok=True)
        self.repo_id = repo_id
        self.fps = fps
        self._writer: AttnVideoRecorder | None = None
        self._frames: list[dict[str, Any]] = []
        self._episode_idx: int | None = None

    def start_episode(self, episode_idx: int) -> None:
        self._episode_idx = episode_idx
        self._frames = []
        self._writer = None

    def log_frame(
        self,
        observation_frame: dict[str, Any],
        joint_state: dict[str, Any],
        frame_idx: int,
        timestamp: float | None = None,
    ) -> None:
        if self._episode_idx is None:
            return
        if not joint_state:
            return

        record = {
            "frame_idx": frame_idx,
            "timestamp": timestamp,
            "actions": [_to_serializable(joint_state)],
        }
        self._frames.append(record)

        images_bgr = _extract_images_from_obs_frame(observation_frame)
        if not images_bgr:
            return
        ordered_keys = sorted(images_bgr.keys())
        overlay_keys = ordered_keys[:2]
        overlays = [images_bgr[k] for k in overlay_keys if k in images_bgr]
        combined = _compose_side_by_side(overlays)

        if combined is None:
            return

        if self._writer is None:
            video_name = f"all_cameras_episode_{self._episode_idx}.mp4"
            self._writer = AttnVideoRecorder(self.output_root / video_name, fps=self.fps)
        self._writer.add_frame(combined)

    def finish_episode(self) -> None:
        if self._episode_idx is None:
            return
        values_path = self.output_root / f"episode_values_{self._episode_idx}.json"
        payload = {
            "repo_id": self.repo_id,
            "episode": self._episode_idx,
            "fps": self.fps,
            "frames": self._frames,
        }
        values_path.write_text(json.dumps(payload), encoding="utf-8")

        if self._writer is not None:
            self._writer.close()
            self._writer = None
        self._frames = []
        self._episode_idx = None

    def finalize(self) -> None:
        if self._writer is not None:
            self._writer.close()
            self._writer = None
        self._frames = []
        self._episode_idx = None


""" --------------- record_loop() data flow --------------------------
       [ Robot ]
           V
     [ robot.get_observation() ] ---> raw_obs
           V
     [ robot_observation_processor ] ---> processed_obs
           V
     .-----( ACTION LOGIC )------------------.
     V                                       V
     [ From Teleoperator ]                   [ From Policy ]
     |                                       |
     |  [teleop.get_action] -> raw_action    |   [predict_action]
     |          |                            |          |
     |          V                            |          V
     | [teleop_action_processor]             |          |
     |          |                            |          |
     '---> processed_teleop_action           '---> processed_policy_action
     |                                       |
     '-------------------------.-------------'
                               V
                  [ robot_action_processor ] --> robot_action_to_send
                               V
                    [ robot.send_action() ] -- (Robot Executes)
                               V
                    ( Save to Dataset )
                               V
                  ( Rerun Log / Loop Wait )
"""


@safe_stop_image_writer
def record_loop(
    robot: Robot,
    events: dict,
    fps: int,
    teleop_action_processor: RobotProcessorPipeline[
        tuple[RobotAction, RobotObservation], RobotAction
    ],  # runs after teleop
    robot_action_processor: RobotProcessorPipeline[
        tuple[RobotAction, RobotObservation], RobotAction
    ],  # runs before robot
    robot_observation_processor: RobotProcessorPipeline[
        RobotObservation, RobotObservation
    ],  # runs after robot
    dataset: LeRobotDataset | None = None,
    teleop: Teleoperator | list[Teleoperator] | None = None,
    policy: PreTrainedPolicy | None = None,
    preprocessor: PolicyProcessorPipeline[dict[str, Any], dict[str, Any]] | None = None,
    postprocessor: PolicyProcessorPipeline[PolicyAction, PolicyAction] | None = None,
    simple_recorder: SimpleRecordingManager | None = None,
    control_time_s: int | None = None,
    single_task: str | None = None,
    display_data: bool = False,
    attn_recorder: AttentionRecordingManager | None = None,
):
    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    teleop_arm = teleop_keyboard = None
    if isinstance(teleop, list):
        teleop_keyboard = next((t for t in teleop if isinstance(t, KeyboardTeleop)), None)
        teleop_arm = next(
            (
                t
                for t in teleop
                if isinstance(
                    t,
                    (so100_leader.SO100Leader | so101_leader.SO101Leader | koch_leader.KochLeader),
                )
            ),
            None,
        )

        if not (teleop_arm and teleop_keyboard and len(teleop) == 2 and robot.name == "lekiwi_client"):
            raise ValueError(
                "For multi-teleop, the list must contain exactly one KeyboardTeleop and one arm teleoperator. Currently only supported for LeKiwi robot."
            )

    # Reset policy and processor if they are provided
    if policy is not None and preprocessor is not None and postprocessor is not None:
        policy.reset()
        preprocessor.reset()
        postprocessor.reset()

    timestamp = 0
    frame_idx = 0
    start_episode_t = time.perf_counter()
    while timestamp < control_time_s:
        start_loop_t = time.perf_counter()

        if events["exit_early"]:
            events["exit_early"] = False
            break

        # Get robot observation
        obs = robot.get_observation()

        # Applies a pipeline to the raw robot observation, default is IdentityProcessor
        obs_processed = robot_observation_processor(obs)

        if policy is not None or dataset is not None:
            observation_frame = build_dataset_frame(dataset.features, obs_processed, prefix=OBS_STR)

        # Get action from either policy or teleop
        if policy is not None and preprocessor is not None and postprocessor is not None:
            action_values = predict_action(
                observation=observation_frame,
                policy=policy,
                device=get_safe_torch_device(policy.config.device),
                preprocessor=preprocessor,
                postprocessor=postprocessor,
                use_amp=policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )

            # ここでアテンションオーバーレイを作って録画（複数カメラ対応）
            if attn_recorders is not None and attn_record_base is not None:
                attn_dir, repo_id_for_attn, ep_idx, attn_fps = attn_record_base

                # observation_frame から画像を全部拾う
                images_bgr = _extract_images_from_obs_frame(observation_frame)

                # policy.config.image_features の順でパッチ範囲が保存されているので、その順にキーを並べる
                image_keys_in_order = [k for k in policy.config.image_features if k in images_bgr]
                patch_ranges = getattr(policy.model, "last_image_patch_ranges", None)
                if patch_ranges is None:
                    warn_count = getattr(policy, "_attn_overlay_no_range", 0)
                    if warn_count < 3:
                        logging.warning("Attention overlay: last_image_patch_ranges missing (keys=%s)", image_keys_in_order)
                    setattr(policy, "_attn_overlay_no_range", warn_count + 1)
                else:
                    warn_counts = getattr(policy, "_attn_overlay_warn_counts", {})
                    for cam_idx, cam_key in enumerate(image_keys_in_order):
                        img_bgr = images_bgr[cam_key]
                        img_range = patch_ranges[cam_idx] if cam_idx < len(patch_ranges) else None
                        overlay, debug_reason = _render_attn_overlay(img_bgr, policy, img_range_override=img_range)

                        # デバッグ用に最初の数回だけ警告を出す（カメラ別）
                        warn_counts.setdefault(cam_key, 0)
                        if overlay is None and debug_reason and warn_counts[cam_key] < 5:
                            logging.warning("Attention overlay skipped [%s]: %s", cam_key, debug_reason)
                            warn_counts[cam_key] += 1

                        frame_to_write = overlay if overlay is not None else img_bgr

                        if cam_key not in attn_recorders:
                            safe_key = cam_key.replace(".", "_")
                            attn_path = attn_dir / f"{repo_id_for_attn.replace('/', '_')}_{safe_key}_ep{ep_idx:06d}.mp4"
                            attn_recorders[cam_key] = AttnVideoRecorder(attn_path, attn_fps)
                            logging.info("Attention video[%s] will be written to: %s", cam_key, attn_path)

                        attn_recorders[cam_key].add_frame(frame_to_write)
                    setattr(policy, "_attn_overlay_warn_counts", warn_counts)

            act_processed_policy: RobotAction = make_robot_action(action_values, dataset.features)

        elif policy is None and isinstance(teleop, Teleoperator):
            act = teleop.get_action()

            # Applies a pipeline to the raw teleop action, default is IdentityProcessor
            act_processed_teleop = teleop_action_processor((act, obs))

        elif policy is None and isinstance(teleop, list):
            arm_action = teleop_arm.get_action()
            arm_action = {f"arm_{k}": v for k, v in arm_action.items()}
            keyboard_action = teleop_keyboard.get_action()
            base_action = robot._from_keyboard_to_base_action(keyboard_action)
            act = {**arm_action, **base_action} if len(base_action) > 0 else arm_action
            act_processed_teleop = teleop_action_processor((act, obs))
        else:
            logging.info(
                "No policy or teleoperator provided, skipping action generation."
                "This is likely to happen when resetting the environment without a teleop device."
                "The robot won't be at its rest position at the start of the next episode."
            )
            continue

        # Applies a pipeline to the action, default is IdentityProcessor
        if policy is not None and act_processed_policy is not None:
            action_values = act_processed_policy
            robot_action_to_send = robot_action_processor((act_processed_policy, obs))
        else:
            action_values = act_processed_teleop
            robot_action_to_send = robot_action_processor((act_processed_teleop, obs))

        # Send action to robot
        # Action can eventually be clipped using `max_relative_target`,
        # so action actually sent is saved in the dataset. action = postprocessor.process(action)
        # TODO(steven, pepijn, adil): we should use a pipeline step to clip the action, so the sent action is the action that we input to the robot.
        _sent_action = robot.send_action(robot_action_to_send)

        # Write to dataset
        if dataset is not None:
            action_frame = build_dataset_frame(dataset.features, action_values, prefix=ACTION)
            frame = {**observation_frame, **action_frame, "task": single_task}
            dataset.add_frame(frame)

        if display_data:
            log_rerun_data(observation=obs_processed, action=action_values)

        dt_s = time.perf_counter() - start_loop_t
        precise_sleep(1 / fps - dt_s)

        joint_state = {k: obs.get(k) for k in getattr(robot, "action_features", {}) if k in obs}
        timestamp = time.perf_counter() - start_episode_t
        if attn_recorder is not None and policy is not None:
            attn_recorder.log_frame(
                observation_frame=observation_frame,
                action_values=action_values,
                frame_idx=frame_idx,
                timestamp=timestamp,
            )
        elif simple_recorder is not None and policy is None:
            simple_recorder.log_frame(
                observation_frame=observation_frame,
                joint_state=joint_state,
                frame_idx=frame_idx,
                timestamp=timestamp,
            )
        frame_idx += 1


@parser.wrap()
def record(cfg: RecordConfig) -> LeRobotDataset:
    init_logging()
    logging.info("lerobot_record.py path: %s", __file__)
    logging.info("Policy provided: %s", cfg.policy.pretrained_path if cfg.policy is not None else None)
    logging.info(pformat(asdict(cfg)))
    if cfg.display_data:
        init_rerun(session_name="recording")

    robot = make_robot_from_config(cfg.robot)
    teleop = make_teleoperator_from_config(cfg.teleop) if cfg.teleop is not None else None

    teleop_action_processor, robot_action_processor, robot_observation_processor = make_default_processors()

    dataset_features = combine_feature_dicts(
        aggregate_pipeline_dataset_features(
            pipeline=teleop_action_processor,
            initial_features=create_initial_features(
                action=robot.action_features
            ),  # TODO(steven, pepijn): in future this should be come from teleop or policy
            use_videos=cfg.dataset.video,
        ),
        aggregate_pipeline_dataset_features(
            pipeline=robot_observation_processor,
            initial_features=create_initial_features(observation=robot.observation_features),
            use_videos=cfg.dataset.video,
        ),
    )

    if cfg.resume:
        dataset = LeRobotDataset(
            cfg.dataset.repo_id,
            root=cfg.dataset.root,
            batch_encoding_size=cfg.dataset.video_encoding_batch_size,
        )

        if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            dataset.start_image_writer(
                num_processes=cfg.dataset.num_image_writer_processes,
                num_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            )
        sanity_check_dataset_robot_compatibility(dataset, robot, cfg.dataset.fps, dataset_features)
    else:
        # Create empty dataset or load existing saved episodes
        sanity_check_dataset_name(cfg.dataset.repo_id, cfg.policy)
        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.dataset.fps,
            root=cfg.dataset.root,
            robot_type=robot.name,
            features=dataset_features,
            use_videos=cfg.dataset.video,
            image_writer_processes=cfg.dataset.num_image_writer_processes,
            image_writer_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            batch_encoding_size=cfg.dataset.video_encoding_batch_size,
        )

    # Load pretrained policy
    policy = None if cfg.policy is None else make_policy(cfg.policy, ds_meta=dataset.meta)
    preprocessor = None
    postprocessor = None
    attn_recorder = None
    simple_recorder = None
    attn_root = Path(cfg.dataset.root) if cfg.dataset.root is not None else Path(dataset.root)
    if cfg.policy is not None:
        preprocessor, postprocessor = make_pre_post_processors(
            policy_cfg=cfg.policy,
            pretrained_path=cfg.policy.pretrained_path,
            dataset_stats=rename_stats(dataset.meta.stats, cfg.dataset.rename_map),
            preprocessor_overrides={
                "device_processor": {"device": cfg.policy.device},
                "rename_observations_processor": {"rename_map": cfg.dataset.rename_map},
            },
        )
        attn_recorder = AttentionRecordingManager(
            policy=policy,
            output_root=attn_root / "attn_videos",
            repo_id=cfg.dataset.repo_id,
            fps=cfg.dataset.fps,
        )
    else:
        simple_recorder = SimpleRecordingManager(
            output_root=attn_root / "attn_videos",
            repo_id=cfg.dataset.repo_id,
            fps=cfg.dataset.fps,
        )

    # DAIHEN PATCH: キャリブレーションファイルが存在する場合のみスキップ
    # calibrate=False で既存のキャリブレーションファイルを上書きしない
    robot_has_calibration = robot.calibration_fpath.is_file() if hasattr(robot, 'calibration_fpath') else False
    robot.connect(calibrate=not robot_has_calibration)
    if teleop is not None:
        teleop_has_calibration = teleop.calibration_fpath.is_file() if hasattr(teleop, 'calibration_fpath') else False
        teleop.connect(calibrate=not teleop_has_calibration)

    listener, events = init_keyboard_listener()

    with VideoEncodingManager(dataset):
        recorded_episodes = 0
        while recorded_episodes < cfg.dataset.num_episodes and not events["stop_recording"]:
            log_say(f"Recording episode {dataset.num_episodes}", cfg.play_sounds)
            if attn_recorder is not None:
                attn_recorder.start_episode(dataset.num_episodes)
            if simple_recorder is not None:
                simple_recorder.start_episode(dataset.num_episodes)
            record_loop(
                robot=robot,
                events=events,
                fps=cfg.dataset.fps,
                teleop_action_processor=teleop_action_processor,
                robot_action_processor=robot_action_processor,
                robot_observation_processor=robot_observation_processor,
                teleop=teleop,
                policy=policy,
                preprocessor=preprocessor,
                postprocessor=postprocessor,
                dataset=dataset,
                control_time_s=cfg.dataset.episode_time_s,
                single_task=cfg.dataset.single_task,
                display_data=cfg.display_data,
                attn_recorder=attn_recorder,
                simple_recorder=simple_recorder,
            )
            if attn_recorder is not None:
                attn_recorder.finish_episode()
            if simple_recorder is not None:
                simple_recorder.finish_episode()

            # エピソード終了時に動画ファイルを閉じる
            if attn_recorders is not None:
                for rec in attn_recorders.values():
                    rec.close()

            # Execute a few seconds without recording to give time to manually reset the environment
            # Skip reset for the last episode to be recorded
            if not events["stop_recording"] and (
                (recorded_episodes < cfg.dataset.num_episodes - 1) or events["rerecord_episode"]
            ):
                log_say("Reset the environment", cfg.play_sounds)
                record_loop(
                    robot=robot,
                    events=events,
                    fps=cfg.dataset.fps,
                    teleop_action_processor=teleop_action_processor,
                    robot_action_processor=robot_action_processor,
                    robot_observation_processor=robot_observation_processor,
                    teleop=teleop,
                    control_time_s=cfg.dataset.reset_time_s,
                    single_task=cfg.dataset.single_task,
                    display_data=cfg.display_data,
                    attn_recorder=None,
                    simple_recorder=None,
                )

            if events["rerecord_episode"]:
                log_say("Re-record episode", cfg.play_sounds)
                events["rerecord_episode"] = False
                events["exit_early"] = False
                dataset.clear_episode_buffer()
                continue

            parallel_encoding = sys.platform != "darwin"
            dataset.save_episode(parallel_encoding=parallel_encoding)
            recorded_episodes += 1

    log_say("Stop recording", cfg.play_sounds, blocking=True)

    robot.disconnect()
    if teleop is not None:
        teleop.disconnect()

    if not is_headless() and listener is not None:
        listener.stop()

    if cfg.dataset.push_to_hub:
        dataset.push_to_hub(tags=cfg.dataset.tags, private=cfg.dataset.private)

    if attn_recorder is not None:
        attn_recorder.finalize()
    if simple_recorder is not None:
        simple_recorder.finalize()

    log_say("Exiting", cfg.play_sounds)
    return dataset


def main():
    register_third_party_plugins()
    record()


if __name__ == "__main__":
    main()
