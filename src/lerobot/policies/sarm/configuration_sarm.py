#!/usr/bin/env python

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass, field

from lerobot.configs.policies import PreTrainedConfig
from lerobot.configs.types import PolicyFeature, FeatureType
from lerobot.optim.optimizers import AdamWConfig
from lerobot.optim.schedulers import CosineDecayWithWarmupSchedulerConfig


@PreTrainedConfig.register_subclass("sarm")
@dataclass
class SARMConfig(PreTrainedConfig):
    """Configuration class for SARM (Stage-Aware Reward Modeling)"""
    
    # Visual encoding parameters
    image_dim: int = 512  # CLIP embedding dimension
    num_frames: int = 9  # 1 initial + 8 consecutive frames
    frame_gap: int = 30  # Frame gap between consecutive frames (at 30 fps = 1 second)
    
    # Text encoding parameters
    text_dim: int = 384 
    
    # Joint state parameters
    state_dim: int | None = None  # Auto-detected from dataset if None
    
    # Architecture parameters
    hidden_dim: int = 768  
    num_heads: int = 12  
    num_layers: int = 8  
    num_stages: int = 5  # Number of task stages for classification (auto-updated from annotations if available)
    subtask_names: list | None = None  # List of subtask names (auto-populated from annotations)
    temporal_proportions: list | None = None  # Temporal proportions for each stage (auto-computed from annotations)
    
    # Temporal parameters
    max_length: int = num_frames  # Maximum video sequence length (matches num_frames)
    use_temporal_sampler: bool = True  # Always enable temporal sequence loading
    sampling_mode: str = "sarm"  # Sampling mode: "sarm" or "rewind"
    
    # Training parameters
    batch_size: int = 64
    clip_batch_size: int = 64  # Batch size for CLIP encoding
    gradient_checkpointing: bool = False  # Enable gradient checkpointing
    dropout: float = 0.1  # Dropout rate
    stage_loss_weight: float = 1.0  # Weight for stage classification loss when using subtask annotations
    
    pretrained_model_path: str | None = None
    
    device: str | None = None
    
    # Processor settings
    image_key: str = "observation.images.top"  # Key for image used from the dataset
    task_description: str = "perform the task"  # Default task description

    # Video_features and text_features are generated by the processor from raw images/text, we don't declare them as VISUAL/LANGUAGE here to avoid validation errors
    input_features: dict = field(default_factory=lambda: {
        "state_features": PolicyFeature(shape=(9, 14), type=FeatureType.STATE)  # Example: 7 DOF Ã— 2 arms
    })
    output_features: dict = field(default_factory=lambda: {
        "stage": PolicyFeature(shape=(1,), type=FeatureType.REWARD),
        "progress": PolicyFeature(shape=(1,), type=FeatureType.REWARD)
    })
    
    def __post_init__(self):
        super().__post_init__()
        
        # Add the image_key, the processor will transform this into video_features
        if self.image_key and self.image_key not in self.input_features:
            self.input_features[self.image_key] = PolicyFeature(
                shape=(480, 640, 3),
                type=FeatureType.VISUAL
            )
        
        # Validate configuration
        if self.hidden_dim % self.num_heads != 0:
            raise ValueError(
                f"hidden_dim ({self.hidden_dim}) must be divisible by num_heads ({self.num_heads})"
            )
        
        if self.max_length != self.num_frames:
            raise ValueError(
                f"max_length ({self.max_length}) must equal num_frames ({self.num_frames})"
            )
        
        if self.dropout < 0 or self.dropout >= 1:
            raise ValueError(f"dropout must be in [0, 1), got {self.dropout}")
        
        if self.num_stages < 2:
            raise ValueError(f"num_stages must be at least 2, got {self.num_stages}")
        
        if self.sampling_mode not in ["sarm", "rewind", "custom"]:
            raise ValueError(
                f"sampling_mode must be 'sarm' or 'rewind', got {self.sampling_mode}"
            )
    
    def get_optimizer_preset(self) -> AdamWConfig:
        """Get default optimizer configuration for SARM training."""
        return AdamWConfig(
            lr=5e-5,
            weight_decay=1e-3,
            betas=(0.9, 0.999),
            eps=1e-8,
        )
    
    def get_scheduler_preset(self) -> CosineDecayWithWarmupSchedulerConfig:
        """Get default learning rate scheduler configuration."""
        return CosineDecayWithWarmupSchedulerConfig(
            peak_lr=5e-5,
            decay_lr=5e-6,
            num_warmup_steps=500,
            num_decay_steps=50000,
        )
    
    def validate_features(self) -> None:
        """Validate input and output features."""
        pass
    
    def observation_delta_indices(self, episode_frame_index: int) -> list[int]:
        """Compute delta indices for SARM temporal sampling.
        
        Per SARM paper (Section A.4), the model uses 9 frames:
        - Frame 0: Initial frame of the episode (delta = -episode_frame_index)
        - Frames 1-8: 8 consecutive frames with frame_gap spacing ending at current frame
        
        The dataloader converts these to seconds: delta_seconds = delta / fps
        This means the first delta (-episode_frame_index) becomes -current_time,
        which correctly points to t=0 (the initial frame).
        
        Args:
            episode_frame_index: Current frame index within the episode (0, 1, 2, ...)
            
        Returns:
            9 delta indices: [-episode_frame_index, -(7*gap), -(6*gap), ..., -gap, 0]
        """
        # First delta: negative of current frame index to reach frame 0
        initial_frame_delta = -episode_frame_index
        
        # Remaining 8 deltas: consecutive frames with frame_gap spacing
        num_consecutive = self.num_frames - 1  # 8 frames
        consecutive_deltas = list(range(-self.frame_gap * (num_consecutive - 1), 1, self.frame_gap))
        
        return [initial_frame_delta] + consecutive_deltas
    
    @property
    def action_delta_indices(self) -> None:
        """SARM is a reward model, not an action policy."""
        return None
    
    @property
    def reward_delta_indices(self) -> None:
        """SARM doesn't use delta rewards."""
        return None

