# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Records a dataset. Actions for the robot can be either generated by teleoperation or by a policy.

Example:

```shell
python -m lerobot.record \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58760431541 \
    --robot.cameras="{laptop: {type: opencv, camera_index: 0, width: 640, height: 480}}" \
    --robot.id=black \
    --dataset.repo_id=aliberts/record-test \
    --dataset.num_episodes=2 \
    --dataset.single_task="Grab the cube" \
    # <- Teleop optional if you want to teleoperate to record or in between episodes with a policy \
    # --teleop.type=so100_leader \
    # --teleop.port=/dev/tty.usbmodem58760431551 \
    # --teleop.id=blue \
    # <- Policy optional if you want to record with a policy \
    # --policy.path=${HF_USER}/my_policy \
```

Example recording with bimanual so100:
```shell
python -m lerobot.record \
  --robot.type=bi_so100_follower \
  --robot.left_arm_port=/dev/tty.usbmodem5A460851411 \
  --robot.right_arm_port=/dev/tty.usbmodem5A460812391 \
  --robot.id=bimanual_follower \
  --robot.cameras='{
    left: {"type": "opencv", "index_or_path": 0, "width": 640, "height": 480, "fps": 30},
    top: {"type": "opencv", "index_or_path": 1, "width": 640, "height": 480, "fps": 30},
    right: {"type": "opencv", "index_or_path": 2, "width": 640, "height": 480, "fps": 30}
  }' \
  --teleop.type=bi_so100_leader \
  --teleop.left_arm_port=/dev/tty.usbmodem5A460828611 \
  --teleop.right_arm_port=/dev/tty.usbmodem5A460826981 \
  --teleop.id=bimanual_leader \
  --display_data=true \
  --dataset.repo_id=${HF_USER}/bimanual-so100-handover-cube \
  --dataset.num_episodes=25 \
  --dataset.single_task="Grab and handover the red cube to the other arm"
```
"""

import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from pprint import pformat

from lerobot.cameras import (  # noqa: F401
    CameraConfig,  # noqa: F401
)
from lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.cameras.kinect.configuration_kinect import KinectCameraConfig  # noqa: F401
from lerobot.configs import parser
from lerobot.configs.policies import PreTrainedConfig
from lerobot.datasets.image_writer import safe_stop_image_writer
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.utils import build_dataset_frame, hw_to_dataset_features, write_info
from lerobot.datasets.video_utils import VideoEncodingManager
from lerobot.policies.factory import make_policy
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    bi_so100_follower,
    bi_so101_follower,
    hope_jr,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    bi_so100_leader,
    bi_so101_leader,
    homunculus,
    koch_leader,
    make_teleoperator_from_config,
    so100_leader,
    so101_leader,
)
from lerobot.teleoperators.keyboard.teleop_keyboard import KeyboardTeleop
from lerobot.utils.control_utils import (
    init_keyboard_listener,
    is_headless,
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.utils.robot_utils import busy_wait
from lerobot.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.utils.visualization_utils import _init_rerun, log_rerun_data, register_depth_scale
from lerobot.cameras.depth_defaults import resolve_depth_params


@dataclass
class DatasetRecordConfig:
    # Dataset identifier. By convention it should match '{hf_username}/{dataset_name}' (e.g. `lerobot/test`).
    repo_id: str
    # A short but accurate description of the task performed during the recording (e.g. "Pick the Lego block and drop it in the box on the right.")
    single_task: str
    # Root directory where the dataset will be stored (e.g. 'dataset/path').
    root: str | Path | None = None
    # Limit the frames per second.
    fps: int = 30
    # Number of seconds for data recording for each episode.
    episode_time_s: int | float = 60
    # Number of seconds for resetting the environment after each episode.
    reset_time_s: int | float = 60
    # Number of episodes to record.
    num_episodes: int = 50
    # Encode frames in the dataset into video
    video: bool = True
    # Upload dataset to Hugging Face hub.
    push_to_hub: bool = True
    # Upload on private repository on the Hugging Face hub.
    private: bool = False
    # Add tags to your dataset on the hub.
    tags: list[str] | None = None
    # Number of subprocesses handling the saving of frames as PNG. Set to 0 to use threads only;
    # set to ≥1 to use subprocesses, each using threads to write images. The best number of processes
    # and threads depends on your system. We recommend 4 threads per camera with 0 processes.
    # If fps is unstable, adjust the thread count. If still unstable, try using 1 or more subprocesses.
    num_image_writer_processes: int = 0
    # Number of threads writing the frames as png images on disk, per camera.
    # Too many threads might cause unstable teleoperation fps due to main thread being blocked.
    # Not enough threads might cause low camera fps.
    num_image_writer_threads_per_camera: int = 4
    # Number of episodes to record before batch encoding videos
    # Set to 1 for immediate encoding (default behavior), or higher for batched encoding
    video_encoding_batch_size: int = 1

    def __post_init__(self):
        if self.single_task is None:
            raise ValueError("You need to provide a task as argument in `single_task`.")


@dataclass
class ColorizeConfig:
    # Offline depth colorization overrides applied to all depth streams unless a camera config overrides them
    colormap: str = "JET"
    min_depth: float | None = None  # meters
    max_depth: float | None = None  # meters
    # depth_scale is device-managed and not user-overridable


@dataclass
class RecordConfig:
    robot: RobotConfig
    dataset: DatasetRecordConfig
    # Whether to control the robot with a teleoperator
    teleop: TeleoperatorConfig | None = None
    # Whether to control the robot with a policy
    policy: PreTrainedConfig | None = None
    # Display all cameras on screen
    display_data: bool = False
    # Use vocal synthesis to read events.
    play_sounds: bool = True
    # Resume recording on an existing dataset.
    resume: bool = False
    # Nested namespace for offline colorization CLI overrides
    colorize: ColorizeConfig | None = None
    # Offline depth colorization CLI overrides (apply to all depth streams unless camera config specifies otherwise)
    colorize_colormap: str | None = None
    colorize_min_depth: float | None = None
    colorize_max_depth: float | None = None
    # depth_scale is device-managed and not user-overridable
    # Optional: display colorized overlay in Rerun
    depth_display_colorized: bool = False
    # Visualization toggles
    viz_depth: bool = False
    # Optional log file path
    log_file: str | None = None
    # Performance logging control
    perf_logging: bool = False
    perf_level: str | None = None

    def __post_init__(self):
        # HACK: We parse again the cli args here to get the pretrained path if there was one.
        policy_path = parser.get_path_arg("policy")
        if policy_path:
            cli_overrides = parser.get_cli_overrides("policy")
            self.policy = PreTrainedConfig.from_pretrained(policy_path, cli_overrides=cli_overrides)
            self.policy.pretrained_path = policy_path

        if self.teleop is None and self.policy is None:
            raise ValueError("Choose a policy, a teleoperator or both to control the robot")

    @classmethod
    def __get_path_fields__(cls) -> list[str]:
        """This enables the parser to load config from the policy using `--policy.path=local/dir`"""
        return ["policy"]


@safe_stop_image_writer
def record_loop(
    robot: Robot,
    events: dict,
    fps: int,
    dataset: LeRobotDataset | None = None,
    teleop: Teleoperator | list[Teleoperator] | None = None,
    policy: PreTrainedPolicy | None = None,
    control_time_s: int | None = None,
    single_task: str | None = None,
    display_data: bool = False,
    viz_depth: bool = False,
):
    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    # Performance stats for control loop (log every 5s)
    perf_last_log_t = time.perf_counter()
    perf_window_s = 5.0
    perf_iter = 0
    perf_fps_sum = 0.0
    perf_fps_sum_sq = 0.0
    perf_fps_min = float("inf")
    perf_fps_max = 0.0

    teleop_arm = teleop_keyboard = None
    if isinstance(teleop, list):
        teleop_keyboard = next((t for t in teleop if isinstance(t, KeyboardTeleop)), None)
        teleop_arm = next(
            (
                t
                for t in teleop
                if isinstance(t, (so100_leader.SO100Leader, so101_leader.SO101Leader, koch_leader.KochLeader))
            ),
            None,
        )

        if not (teleop_arm and teleop_keyboard and len(teleop) == 2 and robot.name == "lekiwi_client"):
            raise ValueError(
                "For multi-teleop, the list must contain exactly one KeyboardTeleop and one arm teleoperator. Currently only supported for LeKiwi robot."
            )

    if policy is not None:
        policy.reset()

    timestamp = 0
    start_episode_t = time.perf_counter()
    while timestamp < control_time_s:
        start_loop_t = time.perf_counter()

        if events["exit_early"]:
            events["exit_early"] = False
            break

        observation = robot.get_observation()

        if policy is not None or dataset is not None:
            # Kinect-only: strip alpha channel at dataset assembly time for color streams
            obs_sanitized = {}
            for k, v in observation.items():
                if isinstance(v, (list, tuple)):
                    obs_sanitized[k] = v
                    continue
                try:
                    import numpy as _np
                    if isinstance(v, _np.ndarray) and v.ndim == 3 and v.shape[-1] == 4 and "kinect" in k:
                        obs_sanitized[k] = v[..., :3]
                    else:
                        obs_sanitized[k] = v
                except Exception:
                    obs_sanitized[k] = v

            observation_frame = build_dataset_frame(dataset.features, obs_sanitized, prefix="observation")

        if policy is not None:
            action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
            action = {key: action_values[i].item() for i, key in enumerate(robot.action_features)}
        elif policy is None and isinstance(teleop, Teleoperator):
            action = teleop.get_action()
        elif policy is None and isinstance(teleop, list):
            arm_action = teleop_arm.get_action()
            arm_action = {f"arm_{k}": v for k, v in arm_action.items()}
            keyboard_action = teleop_keyboard.get_action()
            base_action = robot._from_keyboard_to_base_action(keyboard_action)
            action = {**arm_action, **base_action} if len(base_action) > 0 else arm_action
        else:
            logging.info(
                "No policy or teleoperator provided, skipping action generation. The robot won't be at its rest position at the start of the next episode."
            )
            continue

        sent_action = robot.send_action(action)

        if dataset is not None:
            action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
            frame = {**observation_frame, **action_frame}
            dataset.add_frame(frame, task=single_task)

        if display_data:
            # When depth viz is on, drop RGB streams; when off, drop depth streams
            if viz_depth:
                viz_obs = {k: v for k, v in observation.items() if k.endswith("_depth")}
            else:
                viz_obs = {k: v for k, v in observation.items() if not k.endswith("_depth")}
            log_rerun_data(viz_obs, action)

        dt_s = time.perf_counter() - start_loop_t
        busy_wait(1 / fps - dt_s)

        # Update FPS stats
        if dt_s > 0:
            fps_inst = 1.0 / dt_s
            perf_iter += 1
            perf_fps_sum += fps_inst
            perf_fps_sum_sq += fps_inst * fps_inst
            perf_fps_min = min(perf_fps_min, fps_inst)
            perf_fps_max = max(perf_fps_max, fps_inst)

        now_t = time.perf_counter()
        if (now_t - perf_last_log_t) >= perf_window_s and perf_iter > 0:
            try:
                perf_logger = logging.getLogger("performance")
                n = perf_iter
                fps_avg = perf_fps_sum / n
                mean_sq = perf_fps_sum_sq / n
                fps_var = max(0.0, mean_sq - (fps_avg ** 2))
                fps_std = fps_var ** 0.5
                perf_logger.info(
                    f"Control loop 5s stats — fps(avg={fps_avg:.1f}, std={fps_std:.1f}, min={perf_fps_min:.1f}, max={perf_fps_max:.1f})"
                )
            except Exception:
                pass
            # Reset window
            perf_last_log_t = now_t
            perf_iter = 0
            perf_fps_sum = 0.0
            perf_fps_sum_sq = 0.0
            perf_fps_min = float("inf")
            perf_fps_max = 0.0

        timestamp = time.perf_counter() - start_episode_t


@parser.wrap()
def record(cfg: RecordConfig) -> LeRobotDataset:
    # Initialize logging; performance logger controlled by CLI flags
    import os
    perf_level = cfg.perf_level if cfg.perf_logging else None
    init_logging(
        log_file=Path(cfg.log_file) if cfg.log_file else None,
        perf_level=perf_level,
        console_enabled=False,
        only_perf_logging=True if cfg.perf_logging else False,
    )
    logging.info(pformat(asdict(cfg)))

    if cfg.display_data:
        _init_rerun(session_name="recording")

    robot = make_robot_from_config(cfg.robot)
    teleop = make_teleoperator_from_config(cfg.teleop) if cfg.teleop is not None else None

    action_features = hw_to_dataset_features(robot.action_features, "action", cfg.dataset.video)
    obs_features = hw_to_dataset_features(robot.observation_features, "observation", cfg.dataset.video)


    dataset_features = {**action_features, **obs_features}

    if cfg.resume:
        dataset = LeRobotDataset(
            cfg.dataset.repo_id,
            root=cfg.dataset.root,
            batch_encoding_size=cfg.dataset.video_encoding_batch_size,
        )

        if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            dataset.start_image_writer(
                num_processes=cfg.dataset.num_image_writer_processes,
                num_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            )
        sanity_check_dataset_robot_compatibility(dataset, robot, cfg.dataset.fps, dataset_features)
    else:
        # Create empty dataset or load existing saved episodes
        sanity_check_dataset_name(cfg.dataset.repo_id, cfg.policy)
        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.dataset.fps,
            root=cfg.dataset.root,
            robot_type=robot.name,
            features=dataset_features,
            use_videos=cfg.dataset.video,
            image_writer_processes=cfg.dataset.num_image_writer_processes,
            image_writer_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            batch_encoding_size=cfg.dataset.video_encoding_batch_size,
        )

    # Load pretrained policy
    policy = None if cfg.policy is None else make_policy(cfg.policy, ds_meta=dataset.meta)

    robot.connect()

    # Register per-stream depth scales for correct Rerun depth visualization when display_data is on
    if cfg.display_data and hasattr(robot, "cameras"):
        try:
            for cam_key, cam in robot.cameras.items():
                # RealSense provides device depth scale (meters per unit)
                depth_scale = getattr(cam, "depth_scale", None)
                if depth_scale is not None:
                    register_depth_scale(cam_key, float(depth_scale))
                else:
                    # Kinect float32 in millimeters → 0.001 meters per unit
                    if cam.__class__.__name__.lower().find("kinect") != -1:
                        register_depth_scale(cam_key, 0.001)
        except Exception:
            pass

    # Enrich dataset metadata with per-stream depth colorization parameters and sensor tags
    try:
        # Defaults
        for cam_key, cam in getattr(robot, "cameras", {}).items():
            if not getattr(cam, "use_depth", False):
                continue

            # Feature key in dataset features for depth stream
            depth_ft_key = f"observation.images.{cam_key}_depth"
            if depth_ft_key not in dataset.meta.info["features"]:
                continue

            cam_type = cam.__class__.__name__.lower()
            sensor = "realsense" if "realsense" in cam_type else ("kinect" if "kinect" in cam_type else "kinect")

            cam_cfg = cfg.robot.cameras.get(cam_key, None) if hasattr(cfg.robot, "cameras") else None
            cli_colormap = (cfg.colorize.colormap if (cfg.colorize and cfg.colorize.colormap) else cfg.colorize_colormap)
            cli_min = (cfg.colorize.min_depth if (cfg.colorize and cfg.colorize.min_depth is not None) else cfg.colorize_min_depth)
            cli_max = (cfg.colorize.max_depth if (cfg.colorize and cfg.colorize.max_depth is not None) else cfg.colorize_max_depth)
            params = resolve_depth_params(
                sensor=sensor,
                cam_cfg=cam_cfg,
                cli_colormap=cli_colormap,
                cli_min=cli_min,
                cli_max=cli_max,
                device_scale=getattr(cam, "depth_scale", None),
            )

            # Inject into metadata
            dataset.meta.info["features"][depth_ft_key]["depth_colorization"] = {
                "min_depth": float(params.min_m),
                "max_depth": float(params.max_m),
                "colormap": str(params.colormap),
                **({"depth_scale": float(params.meters_per_unit)} if params.meters_per_unit is not None else {}),
                "sensor": sensor,
            }

        # Persist updated metadata
        write_info(dataset.meta.info, dataset.meta.root)
    except Exception as e:
        logging.warning(f"Failed to annotate depth colorization metadata: {e}")

    # Annotate sensor type for color features as well (used for offline processing decisions)
    try:
        for cam_key, cam in getattr(robot, "cameras", {}).items():
            cam_type = cam.__class__.__name__.lower()
            sensor = "realsense" if "realsense" in cam_type else ("kinect" if "kinect" in cam_type else "unknown")
            color_ft_key = f"observation.images.{cam_key}"
            if color_ft_key in dataset.meta.info["features"]:
                dataset.meta.info["features"][color_ft_key]["sensor"] = sensor
                # If Kinect and resize target exists in camera config, record it for offline processing
                cam_cfg = cfg.robot.cameras.get(cam_key, None) if hasattr(cfg.robot, "cameras") else None
                if sensor == "kinect" and cam_cfg is not None:
                    rw = getattr(cam_cfg, "resize_width", None)
                    rh = getattr(cam_cfg, "resize_height", None)
                    if rw is not None and rh is not None:
                        dataset.meta.info["features"][color_ft_key]["resize"] = {"width": int(rw), "height": int(rh)}
        write_info(dataset.meta.info, dataset.meta.root)
    except Exception:
        pass

    # Rerun registration for scales and optional overlay in record mode
    if cfg.display_data and hasattr(robot, "cameras"):
        try:
            for cam_key, cam in robot.cameras.items():
                cam_type = cam.__class__.__name__.lower()
                sensor = "realsense" if "realsense" in cam_type else ("kinect" if "kinect" in cam_type else "kinect")
                cam_cfg = cfg.robot.cameras.get(cam_key, None) if hasattr(cfg.robot, "cameras") else None
                cli_colormap = (cfg.colorize.colormap if (cfg.colorize and cfg.colorize.colormap) else cfg.colorize_colormap)
                cli_min = (cfg.colorize.min_depth if (cfg.colorize and cfg.colorize.min_depth is not None) else cfg.colorize_min_depth)
                cli_max = (cfg.colorize.max_depth if (cfg.colorize and cfg.colorize.max_depth is not None) else cfg.colorize_max_depth)
                params = resolve_depth_params(
                    sensor=sensor,
                    cam_cfg=cam_cfg,
                    cli_colormap=cli_colormap,
                    cli_min=cli_min,
                    cli_max=cli_max,
                    device_scale=getattr(cam, "depth_scale", None),
                )
                if params.meters_per_unit is not None:
                    register_depth_scale(cam_key, params.meters_per_unit)
        except Exception:
            pass
    if teleop is not None:
        teleop.connect()

    listener, events = init_keyboard_listener()

    with VideoEncodingManager(dataset):
        recorded_episodes = 0
        while recorded_episodes < cfg.dataset.num_episodes and not events["stop_recording"]:
            log_say(f"Recording episode {dataset.num_episodes}", cfg.play_sounds)
            record_loop(
                robot=robot,
                events=events,
                fps=cfg.dataset.fps,
                teleop=teleop,
                policy=policy,
                dataset=dataset,
                control_time_s=cfg.dataset.episode_time_s,
                single_task=cfg.dataset.single_task,
                display_data=cfg.display_data,
                viz_depth=cfg.viz_depth,
            )

            # Execute a few seconds without recording to give time to manually reset the environment
            # Skip reset for the last episode to be recorded
            if not events["stop_recording"] and (
                (recorded_episodes < cfg.dataset.num_episodes - 1) or events["rerecord_episode"]
            ):
                log_say("Reset the environment", cfg.play_sounds)
                record_loop(
                    robot=robot,
                    events=events,
                    fps=cfg.dataset.fps,
                    teleop=teleop,
                    control_time_s=cfg.dataset.reset_time_s,
                    single_task=cfg.dataset.single_task,
                    display_data=cfg.display_data,
                    viz_depth=cfg.viz_depth,
                )

            if events["rerecord_episode"]:
                log_say("Re-record episode", cfg.play_sounds)
                events["rerecord_episode"] = False
                events["exit_early"] = False
                dataset.clear_episode_buffer()
                continue

            dataset.save_episode()
            recorded_episodes += 1

            # # Print performance summary for the episode - DISABLED FOR PERFORMANCE
            # print("\n\nEpisode Performance Summary:")
            # print(f"Episode {dataset.num_episodes - 1} completed")

            # # Print camera statistics if available
            # if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            #     for cam_name, cam in robot.cameras.items():
            #         print(f"\nCamera '{cam_name}':")
            #         if hasattr(cam, "_colorization_error_count"):
            #             total = cam._colorization_success_count + cam._colorization_error_count
            #             if total > 0:
            #                 error_rate = (cam._colorization_error_count / total) * 100
            #                 print(
            #                     f"  Depth colorization: {cam._colorization_success_count} successes, {cam._colorization_error_count} errors ({error_rate:.1f}% error rate)"
            #                 )
            # print("-" * 60)

    log_say("Stop recording", cfg.play_sounds, blocking=True)

    robot.disconnect()
    if teleop is not None:
        teleop.disconnect()

    if not is_headless() and listener is not None:
        listener.stop()

    if cfg.dataset.push_to_hub:
        dataset.push_to_hub(tags=cfg.dataset.tags, private=cfg.dataset.private)

    log_say("Exiting", cfg.play_sounds)
    return dataset


if __name__ == "__main__":
    record()
