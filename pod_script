cd workspace/

# Install miniforge
# TODO figure out uname
   uname -m
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh -b -u -p /workspace/miniconda
   bash Miniconda3-latest-Linux-x86_64.sh -b -u -p /workspace/miniconda

wget "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
bash Miniforge3-$(uname)-$(uname -m).sh

# set up environment
conda create -y -n lerobot python=3.10
conda activate lerobot
conda install ffmpeg -c conda-forge

# If libsvtav1 is not supported (check supported encoders with ffmpeg -encoders), you can
conda install ffmpeg=7.1.1 -c conda-forge

# Install lerobot
git clone https://github.com/huggingface/lerobot.git
cd lerobot
pip install -e .
pip install 'lerobot[feetech]'
pip install -e ".[feetech]"
pip install -e ".[smolvla]"
# WHAT IS THIS?
conda create -y -n lerobot python=3.10
shell init

wandb login
huggingface-cli whoami
huggingface-cli login

tmux new -s "train"

conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y


# TRAIN
python src/lerobot/scripts/lerobot_train.py \
  --policy.path=lerobot/smolvla_base \
  --dataset.repo_id=mthirumalai/so101-picknplace3 \
  --batch_size=64 \
  --steps=20000 \
  --output_dir=outputs/train/my_smolvla \
  --job_name=my_smolvla_training \
  --policy.device=cuda \
  --wandb.enable=true \
  --policy.repo_id=mthirumalai/so101-picknplace3.smolvla-policy1  \
  --rename_map='{"observation.images.front": "observation.images.camera1", "observation.images.side": "observation.images.camera2"}' \
	> your_log_file.log 2>&1 &

# PUSH TO HUGGINGFACE HUB
python src/lerobot/scripts/push_to_hub.py \
  --local_dir=outputs/train/my_smolvla/checkpoints/last/pretrained_model \
  --repo_id=mthirumalai/so101-picknplace3.smolvla-policy1


lerobot-record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \ # <- Use your port
  --robot.id=my_blue_follower_arm \ # <- Use your robot id
  --robot.cameras="{ front: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}}" \ # <- Use your cameras
  --dataset.single_task="Grasp a lego block and put it in the bin." \ # <- Use the same task description you used in your dataset recording
  --dataset.repo_id=${HF_USER}/eval_DATASET_NAME_test \  # <- This will be the dataset name on HF Hub
  --dataset.episode_time_s=50 \
  --dataset.num_episodes=10 \
  # <- Teleop optional if you want to teleoperate in between episodes \
  # --teleop.type=so100_leader \
  # --teleop.port=/dev/ttyACM0 \
  # --teleop.id=my_red_leader_arm \
  --policy.path=HF_USER/FINETUNE_MODEL_NAME # <- Use your fine-tuned model



   39  lerobot-train   --dataset.repo_id=mthirumalai/so101-picknplace3   --policy.type=act_so101   --env.type=so101_real   --output_dir=outputs/train/so101_picknplace3_act   --policy.device=cuda   --wandb.enable=true   --wandb.project=so101_picknplace   --policy.push_to_hub=true   --policy.repo_id=mthirumalai/so101-picknplace3-policy   --save_freq=20000
