# 如何更换成更复杂的任务

本指南介绍如何在 LeRobot HIL-SERL 中配置和使用更复杂的任务。

## 方法概览

1. **修改配置文件** - 如果 gym_hil 有更多预定义任务
2. **自定义环境类** - 创建自己的复杂任务逻辑
3. **修改奖励函数** - 增加任务难度和复杂度
4. **使用真实机器人** - 更灵活的任务配置

## 方法 1: 修改配置文件（最简单）

如果 `gym_hil` 包提供了更多任务，只需修改配置文件中的 `task` 字段：

```json
{
  "env": {
    "name": "gym_hil",
    "task": "PandaPickAndPlace-v0",  // 更换为更复杂的任务
    "fps": 10
  }
}
```

## 方法 2: 创建自定义复杂任务环境（推荐）

### 步骤 1: 创建自定义环境类

创建一个新文件 `src/lerobot/rl/complex_task_env.py`：

```python
import numpy as np
import gymnasium as gym
from typing import Any
from lerobot.rl.gym_manipulator import RobotEnv
from lerobot.utils.constants import OBS_IMAGES, OBS_STATE
from lerobot.teleoperators.utils import TeleopEvents

class ComplexTaskEnv(RobotEnv):
    """复杂任务环境示例：抓取并放置到目标位置"""
    
    def __init__(
        self,
        robot,
        use_gripper: bool = True,
        display_cameras: bool = False,
        reset_pose: list[float] | None = None,
        reset_time_s: float = 5.0,
        target_position: np.ndarray | None = None,
    ):
        super().__init__(robot, use_gripper, display_cameras, reset_pose, reset_time_s)
        
        # 任务特定参数
        self.target_position = target_position if target_position is not None else np.array([0.5, 0.0, 0.3])
        self.object_position = None  # 物体当前位置
        self.gripper_closed = False
        self.object_grasped = False
        
    def reset(
        self, *, seed: int | None = None, options: dict[str, Any] | None = None
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        """重置环境并初始化任务状态"""
        obs, info = super().reset(seed=seed, options=options)
        
        # 重置任务特定状态
        self.object_position = np.array([0.3, 0.0, 0.1])  # 物体初始位置
        self.gripper_closed = False
        self.object_grasped = False
        
        return obs, info
    
    def _compute_reward(self, obs: dict[str, Any]) -> float:
        """计算复杂任务的奖励"""
        reward = 0.0
        
        # 获取末端执行器位置（从观测中提取或通过前向运动学计算）
        ee_position = self._get_end_effector_position(obs)
        
        # 阶段1: 接近物体（奖励距离减少）
        if not self.object_grasped:
            distance_to_object = np.linalg.norm(ee_position - self.object_position)
            reward += -0.1 * distance_to_object  # 距离惩罚
            
            # 如果足够接近，给予奖励
            if distance_to_object < 0.05:
                reward += 0.5
        
        # 阶段2: 抓取物体（检查夹爪状态）
        if self.gripper_closed and not self.object_grasped:
            distance_to_object = np.linalg.norm(ee_position - self.object_position)
            if distance_to_object < 0.03:
                self.object_grasped = True
                reward += 1.0  # 成功抓取奖励
        
        # 阶段3: 移动到目标位置
        if self.object_grasped:
            # 物体位置跟随末端执行器
            self.object_position = ee_position.copy()
            distance_to_target = np.linalg.norm(ee_position - self.target_position)
            reward += -0.1 * distance_to_target
            
            # 如果到达目标位置，给予大奖励
            if distance_to_target < 0.05:
                reward += 5.0  # 任务完成奖励
                return reward, True  # 任务完成
        
        return reward, False
    
    def _get_end_effector_position(self, obs: dict[str, Any]) -> np.ndarray:
        """从观测中提取末端执行器位置"""
        # 这里需要根据你的机器人配置实现
        # 可以使用前向运动学或从观测状态中直接读取
        state = obs.get(OBS_STATE, None)
        if state is not None and len(state) >= 3:
            # 假设前3个元素是末端执行器位置
            return state[:3]
        # 默认返回一个位置
        return np.array([0.0, 0.0, 0.0])
    
    def step(self, action) -> tuple[dict[str, np.ndarray], float, bool, bool, dict[str, Any]]:
        """执行一步，包含复杂任务逻辑"""
        # 检查夹爪动作
        if self.use_gripper and len(action) > 3:
            gripper_action = action[-1]
            self.gripper_closed = gripper_action > 0.5
        
        # 执行基础步骤
        obs, base_reward, terminated, truncated, info = super().step(action)
        
        # 计算任务特定奖励
        task_reward, task_done = self._compute_reward(obs)
        total_reward = base_reward + task_reward
        
        # 更新终止条件
        terminated = terminated or task_done
        
        # 添加任务信息
        info["object_grasped"] = self.object_grasped
        info["distance_to_target"] = np.linalg.norm(
            self._get_end_effector_position(obs) - self.target_position
        )
        
        return obs, total_reward, terminated, truncated, info
```

### 步骤 2: 修改环境创建函数

在 `gym_manipulator.py` 中修改 `make_robot_env` 函数，添加对新环境的支持：

```python
def make_robot_env(cfg: HILSerlRobotEnvConfig) -> tuple[gym.Env, Any]:
    # ... 现有代码 ...
    
    # 如果是复杂任务，使用自定义环境
    if cfg.task == "ComplexPickAndPlace":
        from lerobot.rl.complex_task_env import ComplexTaskEnv
        env = ComplexTaskEnv(
            robot=robot,
            use_gripper=use_gripper,
            display_cameras=display_cameras,
            reset_pose=reset_pose,
        )
        return env, teleop_device
    
    # ... 其他代码 ...
```

## 方法 3: 通过奖励分类器实现复杂任务

对于视觉任务，可以使用奖励分类器来判断任务完成情况：

```json
{
  "env": {
    "processor": {
      "reward_classifier": {
        "pretrained_path": "your_reward_classifier_model",
        "success_threshold": 0.7,
        "success_reward": 10.0  // 复杂任务可以设置更高的奖励
      },
      "reset": {
        "control_time_s": 30.0,  // 复杂任务需要更长时间
        "terminate_on_success": true
      }
    }
  }
}
```

## 方法 4: 多阶段任务配置

对于需要多个步骤的复杂任务，可以在配置中设置：

```json
{
  "env": {
    "processor": {
      "reset": {
        "control_time_s": 20.0,  // 增加时间限制
        "terminate_on_success": true
      },
      "gripper": {
        "use_gripper": true,
        "gripper_penalty": -0.01  // 调整夹爪惩罚
      }
    }
  }
}
```

## 复杂任务示例

### 示例 1: 抓取并放置

- **阶段1**: 接近物体 → 奖励：距离减少
- **阶段2**: 抓取物体 → 奖励：成功抓取
- **阶段3**: 移动到目标 → 奖励：距离减少
- **阶段4**: 放置物体 → 奖励：任务完成

### 示例 2: 双手协作任务

需要配置两个机器人环境，分别控制左右手。

### 示例 3: 顺序操作任务

- 打开抽屉
- 抓取物体
- 关闭抽屉
- 放置物体

## 配置示例

完整的复杂任务配置示例：

```json
{
  "env": {
    "name": "gym_hil",
    "task": "ComplexPickAndPlace",
    "fps": 10,
    "processor": {
      "control_mode": "keyboard",
      "gripper": {
        "use_gripper": true,
        "gripper_penalty": -0.01
      },
      "reset": {
        "control_time_s": 25.0,
        "terminate_on_success": true
      },
      "reward_classifier": {
        "pretrained_path": "path/to/complex_task_classifier",
        "success_threshold": 0.8,
        "success_reward": 10.0
      }
    }
  },
  "dataset": {
    "repo_id": "username/complex_task_dataset",
    "task": "pick_and_place",
    "num_episodes_to_record": 50
  }
}
```

## 训练建议

1. **从简单开始**: 先确保基础任务工作正常
2. **逐步增加复杂度**: 每次只增加一个难度维度
3. **调整超参数**: 复杂任务可能需要：
   - 更长的训练时间
   - 更大的缓冲区
   - 更长的episode长度
   - 不同的学习率

4. **使用奖励塑形**: 为中间步骤提供奖励，帮助学习

## 调试技巧

1. **可视化奖励**: 在训练过程中监控奖励曲线
2. **检查任务状态**: 在 info 字典中添加调试信息
3. **测试重置**: 确保环境可以正确重置
4. **验证动作空间**: 确保动作在合理范围内

## 下一步

- 查看 [HIL-SERL 文档](hilserl.mdx) 了解更多配置选项
- 参考 [奖励函数设置指南](#) 了解如何设计奖励
- 查看示例代码了解最佳实践

