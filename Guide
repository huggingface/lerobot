# =============================================================================
# SETUP
# =============================================================================
# Set USB permissions (run every time after reboot or USB reconnect)
sudo chmod 666 /dev/ttyACM0 /dev/ttyACM1 /dev/ttyACM2

# =============================================================================
# TELEOPERATION (Host System)
# =============================================================================
conda activate lerobot

lerobot-teleoperate \
    --robot.type=xlerobot \
    --teleop.type=xlerobot_vr \
    --display_data=true

# =============================================================================
# RECORDING (Host System)
# =============================================================================
conda activate lerobot

lerobot-record \
    --robot.type=xlerobot \
    --teleop.type=xlerobot_vr \
    --dataset.repo_id=Odog16/ob15_packing_box \
    --dataset.single_task="place blocks in box" \
    --dataset.num_episodes=10 \
    --dataset.fps=30 \
    --display_data=true \
    --dataset.push_to_hub=true \
    --resume=true


 ssh -L 9876:localhost:9876 jetson@192.168.0.104

# =============================================================================
# REMOTE VISUALIZATION (External PC)
# =============================================================================
# Install rerun: pip3 install rerun-sdk
rerun --serve-web --web-viewer-port 9090 --connect "rerun+http://192.168.0.104:9876/proxy"
# Open browser: http://localhost:9090

# =============================================================================
# HUGGINGFACE SETUP
# =============================================================================
huggingface-cli login

# =============================================================================
# TRAINING (Run on PC with GPU)
# =============================================================================
lerobot-train \
    --dataset.repo_id=Odog16/ob15_packing_box \
    --policy.type=act \
    --output_dir=outputs/train/act_xlerobot \
    --policy.device=cuda

# Upload trained policy:
hf upload Odog16/xlerobot_policy \
    outputs/train/act_xlerobot/checkpoints/last/pretrained_model


# =============================================================================
# POLICY EVALUATION
# =============================================================================

# OPTION 1: Async Inference (RECOMMENDED - solves CUDA OOM)
# Run policy server on PC with GPU, robot client on Jetson
# This avoids CUDA OOM by running the model on a machine with more GPU memory
#
# ============================================================================

# On PC (training machine with GPU):
# 1. Install async dependencies: pip install -e ".[async]"
# 2. Find your PC's IP address (e.g., run: ip addr show or ifconfig on Linux/Mac, ipconfig on Windows)
#    Example: if your PC's IP is 192.168.0.106
# 3. Start policy server (use 0.0.0.0 to listen on all interfaces):
python -m lerobot.async_inference.policy_server \
    --host=0.0.0.0 \
    --port=8080

# On Jetson (robot):
# 1. Install async dependencies: pip install -e ".[async]"
# 2. Replace 192.168.0.106 with your PC's actual IP address (from step 2 above)
# 3. Run robot client (connects to PC policy server):
#
# REQUIRED parameters:
#   --policy_type (supported: act, smolvla, diffusion, tdmpc, vqbet, pi0, pi05, groot)
#   --pretrained_name_or_path, --actions_per_chunk
#   --robot.type
#   --server_address (if not using default localhost:8080)
#   --policy_device (if not using default cpu)
#   --task (if not empty)
#
# NOTE: For groot policies, --actions_per_chunk must be <= 16 (groot has a hardcoded limit)
#
# OPTIONAL parameters (have defaults, can be omitted):
#   --robot.port1 (default: /dev/ttyACM1) - shown for clarity, omit if using default
#   --robot.port2 (default: /dev/ttyACM2) - shown for clarity, omit if using default
#   --robot.port3 (default: /dev/ttyACM0) - shown for clarity, omit if using default
#   --robot.cameras (default: includes /dev/video4, /dev/video2, /dev/video0 with fourcc: MJPG)
#                    - shown to explicitly set fourcc: MJPG (important for camera compatibility)
# Option 1: Use latest action only (what you probably want)
--aggregate_fn_name="latest_only"

# Option 2: Weighted average (favors newer actions)
--aggregate_fn_name="weighted_average"

# Option 3: Simple average
--aggregate_fn_name="average"

# Option 4: Conservative (favors older actions)
--aggregate_fn_name="conservative"



conda activate lerobot
python -m lerobot.async_inference.robot_client \
    --server_address=192.168.0.106:8080 \
    --robot.type=xlerobot \
    --task="packing_box" \
    --policy_type=act \
    --pretrained_name_or_path=Odog16/act_200k_ob15_packing_box_policy \
    --policy_device=cuda \
    --actions_per_chunk=120 \
    --chunk_size_threshold=0.5 \
    --aggregate_fn_name="latest_only" \
    --fps=30

python -m lerobot.async_inference.robot_client \
    --server_address=192.168.0.106:8080 \
    --robot.type=xlerobot \
    --task="packing_box" \
    --policy_type=smolvla \
    --pretrained_name_or_path=Odog16/smolvla_ob15_packing_box_policy_2 \
    --policy_device=cuda \
    --actions_per_chunk=80 \
    --chunk_size_threshold=0.3 \
    --aggregate_fn_name="weighted_average" \
    --fps=30




    --robot.port1=/dev/ttyACM1 \
    --robot.port2=/dev/ttyACM2 \
    --robot.port3=/dev/ttyACM0 \
    --robot.cameras="{head: {type: opencv, index_or_path: /dev/video4, width: 640, height: 480, fps: 30, fourcc: MJPG}, left_wrist: {type: opencv, index_or_path: /dev/video2, width: 640, height: 480, fps: 30, fourcc: MJPG}, right_wrist: {type: opencv, index_or_path: /dev/video0, width: 640, height: 480, fps: 30, fourcc: MJPG}}" \



# OPTION 2: Direct evaluation (may hit CUDA OOM on Jetson)
# 
# MODEL SIZE LIMITS FOR JETSON ORIN NANO (8GB):
# - Total system RAM: 8GB (shared with GPU)
# - Available GPU memory: ~4-5GB (after OS overhead)
# - Model size limits:
#   * Small models (<500MB): Should work fine
#   * Medium models (500MB-1GB): May work with optimizations (mixed precision, smaller batch)
#   * Large models (>1GB): Likely to hit OOM (like your 1.2GB SmolVLA model)
# 
# TROUBLESHOOTING CUDA OOM:
# If you get "NvMapMemAllocInternalTagged: error 12" or "CUDACachingAllocator" errors,
# the model (1.2GB) is too large for Jetson GPU memory. Use Option 1 (async) instead.
# 
# For larger Jetson models:
# - Jetson Orin AGX (32GB/64GB): Can handle models up to ~10-20GB
# - Jetson Orin NX (16GB): Can handle models up to ~5-8GB
# 
# DOCKER PERMISSIONS:
# If you get "permission denied" or "unknown server OS" errors, add user to docker group:
#   sudo usermod -aG docker $USER
#   newgrp docker  # or logout/login
# The "unknown server OS" error is often a misleading message when Docker can't connect due to permissions.
# Verify docker access: docker version (should work without sudo after adding to docker group)
docker run --runtime nvidia --env NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics \
  -it --rm --network host --shm-size=4g \
  --volume /tmp/argus_socket:/tmp/argus_socket \
  --volume /etc/enctune.conf:/etc/enctune.conf \
  --volume /etc/nv_tegra_release:/etc/nv_tegra_release \
  --volume /tmp/nv_jetson_model:/tmp/nv_jetson_model \
  --volume /var/run/dbus:/var/run/dbus \
  --volume /var/run/avahi-daemon/socket:/var/run/avahi-daemon/socket \
  --device /dev/snd -e PULSE_SERVER=unix:/run/user/1000/pulse/native \
  -v /run/user/1000/pulse:/run/user/1000/pulse \
  --device /dev/bus/usb \
  --device /dev/video0 --device /dev/video1 --device /dev/video2 \
  --device /dev/video3 --device /dev/video4 --device /dev/video5 \
  --device /dev/ttyACM0 --device /dev/ttyACM1 --device /dev/ttyACM2 \
  -v /home/jetson/lerobot:/opt/lerobot -w /opt/lerobot \
  dustynv/lerobot:r36.4.0 \
  bash -c "set +H && cd /opt/lerobot && pip install --index-url https://pypi.org/simple --force-reinstall 'numpy<2' && pip install --index-url https://pypi.org/simple 'datasets>=4.0.0,<4.2.0' 'diffusers>=0.27.2,<0.36.0' 'huggingface-hub[hf-transfer,cli]>=0.34.2,<0.36.0' 'accelerate>=1.10.0,<2.0.0' 'setuptools>=71.0.0,<81.0.0' 'cmake>=3.29.0.1,<4.2.0' 'einops>=0.8.0,<0.9.0' 'opencv-python-headless>=4.9.0,<4.13.0' 'av>=15.0.0,<16.0.0' 'jsonlines>=4.0.0,<5.0.0' 'packaging>=24.2,<26.0' 'pynput>=1.7.7,<1.9.0' 'pyserial>=3.5,<4.0' 'wandb>=0.20.0,<0.22.0' 'draccus==0.10.0' 'gymnasium>=1.1.1,<2.0.0' 'rerun-sdk>=0.24.0,<0.27.0' 'deepdiff>=7.0.1,<9.0.0' 'imageio[ffmpeg]>=2.34.0,<3.0.0' 'termcolor>=2.4.0,<4.0.0' 'transformers>=4.53.0,<5.0.0' 'num2words>=0.5.14,<0.6.0' 'safetensors>=0.4.3,<1.0.0' 'feetech-servo-sdk>=1.0.0,<2.0.0' && pip install --index-url https://pypi.org/simple --no-deps -e . && pip install --index-url https://pypi.org/simple --force-reinstall 'numpy<2' && python -c 'import torch; torch.cuda.empty_cache(); assert torch.cuda.is_available(), \"CUDA not available\"; print(f\"CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0)}, Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")' && python -m lerobot.scripts.lerobot_record --robot.type=xlerobot --dataset.repo_id=Odog16/eval_ob15_packing_box --dataset.single_task=packing_box --dataset.num_episodes=5 --display_data=false --policy.path=Odog16/smolvla_ob15_packing_box_policy_1"




# Split dataset and push to hub
# NOTE: Split names must use underscores, not spaces (Hugging Face repo ID requirement)
# If you get "FileExistsError", delete existing split directories first:
# rm -rf ~/.cache/huggingface/lerobot/Odog16/ob15_packing_box_place_blocks_in_box ~/.cache/huggingface/lerobot/Odog16/ob15_packing_box_pick_box_and_place

lerobot-edit-dataset \
    --repo_id Odog16/ob15_packing_box \
    --operation.type split \
    --operation.splits '{"place_blocks_in_box": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74], "pick_box_and_place_blocks_in_box": [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]}' \
    --push_to_hub true

# This creates two datasets on Hugging Face Hub:
# - Odog16/ob15_packing_box_place_blocks_in_box (75 episodes)
# - Odog16/ob15_packing_box_pick_box_and_place (40 episodes)

# =============================================================================
# DATASET CLEANUP - Delete Unwanted Episodes
# =============================================================================
# Delete specific episodes and keep only the ones you want
# Example: Keep only episodes 740, 1-15 (delete all others)

# First, inspect the dataset to see total episodes:
# python -c "from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata; meta = LeRobotDatasetMetadata('Odog16/packing_box'); print(f'Total episodes: {meta.total_episodes}')"

# Then delete unwanted episodes (save to new dataset to preserve original):
# NOTE: You need to specify ALL episodes to DELETE, not the ones to keep
# If you want to keep episodes [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 740],
# you need to delete all other episodes (e.g., if total is 741, delete [0, 16-739])

# Example: Delete episodes 0, 16-739 (keeping 1-15 and 740)
# lerobot-edit-dataset \
#     --repo_id Odog16/packing_box \
#     --new_repo_id Odog16/packing_box_cleaned \
#     --operation.type delete_episodes \
#     --operation.episode_indices "[0, 16, 17, 18, ...]" \
#     --push_to_hub true

# Or delete episodes and overwrite original (WARNING: permanent deletion):
 lerobot-edit-dataset \
     --repo_id Odog16/ob15_packing_box \
     --operation.type delete_episodes \
     --operation.episode_indices "[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]" \
     --push_to_hub true


# =============================================================================
# GIT WORKFLOW
# =============================================================================
cd /home/jetson/lerobot
git add .
git commit -m "Your message"
git pull origin main
git push origin main
